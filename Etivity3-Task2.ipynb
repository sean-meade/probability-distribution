{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook E-tivity 3 CE4021 Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name:** Jason Coleman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID:** 9539719"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you believe required imports are missing, please contact your moderator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below information to create a Naive Bayes SPAM filter. Test your filter using the messages in new_emails. You may add as many cells as you require to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
    "previous_ham = ['Your activity report','benefits physical activity', 'the importance vows']\n",
    "new_emails = {'spam':['renew your password', 'renew your vows'], 'ham':['benefits of our account', 'the importance of physical activity']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This will be a very naive Naive Bayes classifier. It will follow a so-called `Bag-of-words` approach where I will classify a message as `SPAM` or `HAM` based on the presence of the words in the string. Notably, the bag of words does not consider the context in which the work is used (i.e. you can reorder the words into complete nonsense and get the same result). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Bayes Theorem\n",
    "Let us start with the formula for Bayes Theorem. \n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "* where $P(A \\mid B)$ is the probability of event A happening given that event B has happened.\n",
    "* and $P(B \\mid A)$ is the probability of event B given A.\n",
    "* and $P(A)$ and $P(B)$ are the probabilities of events A and B, respectively.\n",
    "\n",
    "In the context of our spam filter:\n",
    "\n",
    "\n",
    "*  where $A$ could be the event \"The email is spam\".\n",
    "*  and $B$ could be \"The email contains the word 'pAssword'\".\n",
    "\n",
    "### Naive Bayes Classifier (NB)\n",
    "\n",
    "NB is a classification technique based on Bayes' theorem; with the \"naive\" assumption that every pair of features is independent of each other. Even though it is caled naive, it can perform surpisingly well. I worked as an analysts in the AV industry a few years ago and, in the early days, when we had less data (and features) NB worked really well. \n",
    "\n",
    "Reframing this a little, in the context of SPAM/HAM.\n",
    "\n",
    "* Where $S$ represents the event that an email is spam.\n",
    "* and $W$ represents the event that a specific word appears in the email.\n",
    "\n",
    "We can write:\n",
    "\n",
    "$$\n",
    "P(S \\mid W) = \\frac{P(W \\mid S) \\times P(S)}{P(W)}\n",
    "$$\n",
    "\n",
    "\n",
    "*  Where $P(S \\mid W)$ is the probability that an email is spam given that it contains the word $W$.\n",
    "*  and $P(W \\mid S)$ is the probability that the word $W$ appears in a spam email.\n",
    "*  and $P(S)$ is the overall probability (or prior) that any email is spam. This is a `prior`. \n",
    "*  and $P(W)$ is the probability that the word $W$ appears in any email, regardless of it being spam or not. This is a `prior`. \n",
    "\n",
    "#### Working on messages with more than one word\n",
    "\n",
    "Emails are rarely composed of just one word so our implemnentation will need to account for all of the words in the message. So, for a given email message, $m$, consisting of words $w_1$, $w_2$, $\\ldots$, $w_n$, we need to modify the method to account for multiple words. \n",
    "\n",
    "0. We will need to calculate the likelihoods for words being both SPAM or HAM. That is: $P(w_1 | S)$, $P(w_1 | H)$ for each word int he training data. \n",
    "\n",
    "Then, for the training messages:\n",
    "\n",
    "1. Calculate the likelihood of the email being Spam, based on word frequncy:\n",
    "\n",
    "$$\n",
    "P(m | S) = P(w_1 | S) \\times P(w_2 | S) \\times \\ldots \\times P(w_n | S)\n",
    "$$\n",
    "\n",
    "2. Calculate the likelihood of the email being Ham, based on word frequncy:\n",
    "\n",
    "$$\n",
    "P(m | H) = P(w_1 | H) \\times P(w_2 | H) \\times \\ldots \\times P(w_n | H)\n",
    "$$\n",
    "\n",
    "Then, finally, calculate the posterior probabilitites for being Ham and Spam.\n",
    "\n",
    "$$\n",
    "P(S | m) = \\frac{P(m | S) \\times P(S)}{P(m)}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "P(H | m) = \\frac{P(m | H) \\times P(H)}{P(m)}\n",
    "$$\n",
    "\n",
    "* Where $P(S)$ and $P(H)$ are the prior probabilities of a message being spam and ham, respectively (we know this from the initial training set).\n",
    "* and $P(m)$ is the total probability of observing message $m$. \n",
    "\n",
    "Note, I don't need to calculate $P(m)$ if I am comparing the relative probability of $P(S | m)$ and $P(H | m)$ so the method I will implement will actually be the following:\n",
    "\n",
    "$$\n",
    "P(S | m) = P(w_1 | S) \\times P(w_2 | S) \\times \\ldots \\times P(w_n | S) \\times P(S)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "P(H | m) = P(w_1 | H) \\times P(w_2 | H) \\times \\ldots \\times P(w_n | H)\\times P(H)\n",
    "$$\n",
    "\n",
    "#### Classification\n",
    "Then, given a new message, if $P(S | m) > P(H | m)$, I will classify the message as SPAM, else I will classify the message as Ham. \n",
    "\n",
    "By doing this, I am side-stepping using a threshold and just comparing the relative posterior probabilitites for the message being HAM and SPAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "We will need to preprocess the data a little; to get it into the right shape. We will start by getting the components of the Bayes formula: the `priors` and the `likelihoods` for the words within the messages. \n",
    "\n",
    "**Tokenise:** break each message into words. Each word is called a `token`. It's the fundemental unit of classification in a Bag of Words classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to calculate Priors and likelihoods. To do this we first need to construct a vocabulary. \n",
    "\n",
    "**Notes**:\n",
    "\n",
    "* I will reference the steps in the reference implementation\n",
    "* My implementation will use list and dictionary comprehensions to make the code more concise.\n",
    "* I will print intermediate output to aid understanding and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary\n",
    "The prior probability of an email being spam or ham is calculated based on the proportion of spam or ham emails in the training data.\n",
    "\n",
    "1. Create a set called *vocabulary*.\n",
    "2. For each email in the SPAM and HAM training messages, split the words\n",
    "3. Add all words to the set.\n",
    "\n",
    "We now have a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: report, benefits, password, importance, send, account, the, your, vows, us, activity, physical, review, website, our, Your, "
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for email in previous_spam + previous_ham:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        vocabulary.add(word)\n",
    "\n",
    "print('Vocabulary:', end=' ')\n",
    "for word in vocabulary:\n",
    "    print(word, end=', ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probability of an email being spam or ham is calculated based on the proportion of spam or ham emails that exists in the training data.\n",
    "\n",
    "$$\n",
    "\\texttt{prior\\_spam} = \\frac{\\texttt{num\\_spam\\_words}}{\\texttt{total\\_words}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\texttt{prior\\_ham} = \\frac{\\texttt{num\\_ham\\_words}}{\\texttt{total\\_words}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 spam words in the training set.\n",
      "There are 9 ham words in the training set.\n",
      "Total words in the training set: 23\n",
      "Prior Spam: 0.6086956521739131\n",
      "Prior Ham: 0.3913043478260869\n"
     ]
    }
   ],
   "source": [
    "#get words from previous_spam\n",
    "spam_words = []\n",
    "for email in previous_spam:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        spam_words.append(word)\n",
    "num_spam_words = len(spam_words)\n",
    "\n",
    "print (f\"There are {len(spam_words)} spam words in the training set.\")\n",
    "        \n",
    "# get words from previous_ham\n",
    "ham_words = []\n",
    "for email in previous_ham:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        ham_words.append(word)\n",
    "num_ham_words = len(ham_words)\n",
    "\n",
    "print (f\"There are {len(ham_words)} ham words in the training set.\")\n",
    "\n",
    "total_words = len(spam_words) + len(ham_words) # total number of words in spam emails\n",
    "print( f\"Total words in the training set: {total_words}\")\n",
    "\n",
    "prior_spam = num_spam_words / (num_spam_words + num_ham_words)\n",
    "prior_ham = 1 - prior_spam\n",
    "\n",
    "print(f\"Prior Spam: {prior_spam}\")\n",
    "print(f\"Prior Ham: {prior_ham}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Conditional Probabilitites (aka Likelihoods)\n",
    "Here, for each word in the vocabulary, the probability of the word given that an email is spam or ham is calculated. We do this by:\n",
    "\n",
    "1. Counting the occurrences of each word in both the spam and ham emails.\n",
    "2. Apply Laplace Smoothing to avoid zero probabilities. If we don't add this, we will eventually end up multiplying by zero (as the word has never been seen before, the conditional probability willk be zero).\n",
    "3. Calculate the conditional probabilities (likelihoods) for each word being both SAPM and HAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      "{'report', 'benefits', 'password', 'importance', 'send', 'account', 'the', 'your', 'vows', 'us', 'activity', 'physical', 'review', 'website', 'our', 'Your'}\n",
      "\n",
      "Spam/HAM dictionaries are as follows:\n",
      "\n",
      "\tWords\t\tSpam Count \tHam Count\n",
      "\t----------------------------------------\n",
      "\treport    \t 0\t\t 1\n",
      "\tbenefits  \t 0\t\t 1\n",
      "\tpassword  \t 2\t\t 0\n",
      "\timportance\t 0\t\t 1\n",
      "\tsend      \t 3\t\t 0\n",
      "\taccount   \t 1\t\t 0\n",
      "\tthe       \t 0\t\t 1\n",
      "\tyour      \t 3\t\t 0\n",
      "\tvows      \t 0\t\t 1\n",
      "\tus        \t 2\t\t 0\n",
      "\tactivity  \t 0\t\t 2\n",
      "\tphysical  \t 0\t\t 1\n",
      "\treview    \t 1\t\t 0\n",
      "\twebsite   \t 1\t\t 0\n",
      "\tour       \t 1\t\t 0\n",
      "\tYour      \t 0\t\t 1\n",
      "\n",
      "The condtional probabilities (likelihoods) for spam and ham are as follows (.md format):\n",
      "\n",
      "|Words\t\t|P(w\\|S)\t|P(w\\|H)|\n",
      "|------------|--------------|-------------|\n",
      "|report    \t|0.0714|\t\t0.2222|\n",
      "|benefits  \t|0.0714|\t\t0.2222|\n",
      "|password  \t|0.2143|\t\t0.1111|\n",
      "|importance\t|0.0714|\t\t0.2222|\n",
      "|send      \t|0.2857|\t\t0.1111|\n",
      "|account   \t|0.1429|\t\t0.1111|\n",
      "|the       \t|0.0714|\t\t0.2222|\n",
      "|your      \t|0.2857|\t\t0.1111|\n",
      "|vows      \t|0.0714|\t\t0.2222|\n",
      "|us        \t|0.2143|\t\t0.1111|\n",
      "|activity  \t|0.0714|\t\t0.3333|\n",
      "|physical  \t|0.0714|\t\t0.2222|\n",
      "|review    \t|0.1429|\t\t0.1111|\n",
      "|website   \t|0.1429|\t\t0.1111|\n",
      "|our       \t|0.1429|\t\t0.1111|\n",
      "|Your      \t|0.0714|\t\t0.2222|\n"
     ]
    }
   ],
   "source": [
    "# Create dictionaries from the vocabulary to store word counts, initialise the count to zero\n",
    "spam_word_counts = {word: 0 for word in vocabulary}\n",
    "ham_word_counts = {word: 0 for word in vocabulary}\n",
    "print(f\"Vocabulary: \\n{vocabulary}\\n\")\n",
    "\n",
    "# Determine frequencies of words in spam and ham emails (I cosider repeatred words here)\n",
    "for email in previous_spam:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        spam_word_counts[word] += 1\n",
    "\n",
    "for email in previous_ham:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        ham_word_counts[word] += 1\n",
    "\n",
    "print(\"Spam/HAM dictionaries are as follows:\\n\")\n",
    "\n",
    "# print the dictionaries for spam ham in one table\n",
    "print('\\tWords\\t\\tSpam Count \\tHam Count')\n",
    "print('\\t'+'-' * 40)\n",
    "for word in vocabulary:\n",
    "    print(f'\\t{word:10s}\\t{spam_word_counts[word]:2d}\\t\\t{ham_word_counts[word]:2d}')\n",
    "\n",
    "print()\n",
    "\n",
    "#  Preparing the (smoothed) likelihoods\n",
    "alpha = 1 # Laplace smoothing (in this case 'add-one' smoothing) parameter\n",
    "\n",
    "# Compute the likeloods for SPAM/HAM.\n",
    "spam_likelihoods = {word: (count + alpha) / num_spam_words for word, count in spam_word_counts.items()}\n",
    "ham_likelihoods = {word: (count + alpha) / num_ham_words for word, count in ham_word_counts.items()}\n",
    "\n",
    "print(\"The condtional probabilities (likelihoods) for spam and ham are as follows (.md format):\\n\")\n",
    "\n",
    "# print a table of spam_probs and ham_probs\n",
    "print('|Words\\t\\t|P(w\\|S)\\t|P(w\\|H)|')\n",
    "print('|------------|--------------|-------------|')\n",
    "for word in vocabulary:\n",
    "    print(f'|{word:10s}\\t|{spam_likelihoods[word]:.4f}|\\t\\t{ham_likelihoods[word]:.4f}|')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Classifier\n",
    "We can now test the classfier on the test set of data. We use the previously calculated priors and likelihood to calculate the posterior probabilities for both SPAM and HAM and this will allow me to classify new emails as spam or ham. The classification is done by comparing the posterior probability for Ham to that of SPAM: the highest one is the output label.\n",
    "\n",
    "We can assess the accuracy of the classifier against data it has not seen before. We do this with a labelled test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPAM msg: \"renew your password\"\n",
      "\tSpam Posterior Probability: 0.0026619343389529724\n",
      "\tHam Posterior Probability: 0.0005367686527106816\n",
      "renew your password            spam  CORRECT!\n",
      "\n",
      "SPAM msg: \"renew your vows\"\n",
      "\tSpam Posterior Probability: 0.0008873114463176574\n",
      "\tHam Posterior Probability: 0.0010735373054213632\n",
      "renew your vows                ham   WRONG!\n",
      "\n",
      "HAM msg: \"benefits of our account\"\n",
      "\tSpam Posterior Probability: 6.337938902268981e-05\n",
      "\tHam Posterior Probability: 0.0001192819228245959\n",
      "benefits of our account        ham   CORRECT!\n",
      "\n",
      "HAM msg: \"the importance of physical activity\"\n",
      "\tSpam Posterior Probability: 1.1317748039766037e-06\n",
      "\tHam Posterior Probability: 0.00015904256376612786\n",
      "the importance of physical activity ham   CORRECT!\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_total = 0\n",
    "\n",
    "# for all labelled messages in the test set, grouped by label\n",
    "for expected_label, msgs in new_emails.items():\n",
    "    for msg in msgs:\n",
    "\n",
    "        print(f'\\n{expected_label.upper()} msg: \"{msg}\"')\n",
    "\n",
    "        words = msg.split()\n",
    "\n",
    "        # initialise with the priors\n",
    "        spam_posterior_prob = prior_spam\n",
    "        ham_posterior_prob = prior_ham\n",
    "\n",
    "      \n",
    "        # Caculate the posterior probabilities based on the multiplied likelihoods\n",
    "        for word in words:\n",
    "            if word in spam_likelihoods:\n",
    "                spam_posterior_prob *= spam_likelihoods[word]\n",
    "            else:\n",
    "                spam_posterior_prob *= alpha / num_spam_words \n",
    "                \n",
    "            if word in ham_likelihoods:\n",
    "                ham_posterior_prob *= ham_likelihoods[word]\n",
    "            else:\n",
    "                ham_posterior_prob *= alpha / num_ham_words\n",
    "\n",
    "        print(f'\\tSpam Posterior Probability: {spam_posterior_prob}')\n",
    "        print(f'\\tHam Posterior Probability: {ham_posterior_prob}')\n",
    "                \n",
    "        # I think it is better to compare the relative probabilities instead of\n",
    "        # picking a threshold. You always get into a trade-off between false\n",
    "        # positives and false negatives.        \n",
    "        if spam_posterior_prob > ham_posterior_prob:\n",
    "            predicted_label = 'spam'\n",
    "        else:\n",
    "            predicted_label = 'ham'\n",
    "\n",
    "        status = 'WRONG!' if predicted_label != expected_label else 'CORRECT!'\n",
    "        print(f'{msg:30s} {predicted_label:5s} {status}')\n",
    "        \n",
    "        if predicted_label == expected_label:\n",
    "            num_correct += 1\n",
    "\n",
    "        num_total += 1\n",
    "\n",
    "accuracy = num_correct / num_total\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this all work?\n",
    "The approach relies on Bayes' Rule, which relates the probability of a message being SPAM based on prior knowledge (that is, having seen examples of HAM and SPAM). \n",
    "\n",
    "The approach taken relies on pre-processing the data to ensure I have a vocabulary of words and an understanding of their occurrence in terms of being SPAM or HAM (i.e. P(w|S) and P(w|H)). I classify a message by breaking the message up into its constituent words and working on these components, as outlined here. \n",
    "\n",
    "There are two main stages to this process: fitting and prediction.\n",
    "\n",
    "**Fitting:**\n",
    "\n",
    "* Compute Priors: Calculate the prior probabilities, based on the frequency of spam and ham emails in the training data.\n",
    "* Compute Likelihoods: For each word in our vocabulary, compute the likelihood it appears in spam and ham emails. Note that I use Laplace smoothing to make sure that any words not seen in the training set don't zero out the probabilities when making predictions (i.e. multiplying by zero pulls the probability down to zero - this is called Laplacian smoothing and is described in the branch).\n",
    "\n",
    "In our case, for our training data set the Priors are calculated as:\n",
    "\n",
    "$$\n",
    "P(S) = \\frac{14}{23} = 0.6086956521739131\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(H) = \\frac{9}{23} = 0.3913043478260869\n",
    "$$\n",
    "\n",
    "The conditional probabilities (or likelihoods) are calculated to be:\n",
    "\n",
    "| Words       | Fractional P(w\\|S) | P(w\\|S) | Fractional P(w\\|H) | P(w\\|H) |\n",
    "|-------------|-------------------|---------|-------------------|---------|\n",
    "| password    | 3/14              | 0.2143  | 1/9               | 0.1111  |\n",
    "| our         | 1/7               | 0.1429  | 1/9               | 0.1111  |\n",
    "| vows        | 1/14              | 0.0714  | 2/9               | 0.2222  |\n",
    "| us          | 3/14              | 0.2143  | 1/9               | 0.1111  |\n",
    "| the         | 1/14              | 0.0714  | 2/9               | 0.2222  |\n",
    "| benefits    | 1/14              | 0.0714  | 2/9               | 0.2222  |\n",
    "| website     | 1/7               | 0.1429  | 1/9               | 0.1111  |\n",
    "| importance  | 1/14              | 0.0714  | 2/9               | 0.2222  |\n",
    "| activity    | 1/14              | 0.0714  | 1/3               | 0.3333  |\n",
    "| send        | 2/7               | 0.2857  | 1/9               | 0.1111  |\n",
    "| physical    | 1/14              | 0.0714  | 2/9               | 0.2222  |\n",
    "| review      | 1/7               | 0.1429  | 1/9               | 0.1111  |\n",
    "| your        | 2/7               | 0.2857  | 1/9               | 0.1111  |\n",
    "| report      | 1/14              | 0.0714  | 2/9               | 0.2222  |\n",
    "| account     | 1/7               | 0.1429  | 1/9               | 0.1111  |\n",
    "| Your        | 1/14              | 0.0714  | 2/9               | 0.2222  |\n",
    "\n",
    "When calculating posterior probabiltities for spam, unknown words will be assigned, $\\frac{1}{14}$; and for ham, unknown words will be assigned $\\frac{1}{9}$. \n",
    "\n",
    "**Prediction:**\n",
    "\n",
    "* For a new, unlabeled email, break it down into its constituent words.\n",
    "* For each word, multiply the likelihoods, initialising the product with the prior (I do two passes: one assuming the email is spam and separately assuming it's ham). \n",
    "* Apply Bayes Rule. Combine the prior probabilities with the computed likelihoods to get the posterior probabilities that the email is spam or ham. Classify the email based on whichever probability is higher, SPAM or HAM.\n",
    "\n",
    "I then evaluate the accuracy of the model by predicting the labels of a test set of data and measuring how many we predict correctly. \n",
    "\n",
    "Note, you can always re-fit the models as you get more data, improving the system's awareness of what is (and is not) SPAM (see the update method on the python class).\n",
    "\n",
    "### Classify (or Predict the Class) for the message: \"renew your password\"\n",
    "\n",
    "We take the test message, \"renew your password\" and compute the following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(S|\\text{renew your password}) &= P(S) \\times P(renew | S) \\times P(your | S) \\times P(password | S) \\\\\n",
    "    P(S|\\text{renew your password}) &= \\frac{14}{23} \\times \\frac{1}{14} \\times  \\frac{2}{7} \\times \\frac{3}{14}  \\\\\n",
    "    P(S|\\text{renew your password}) &= \\frac{3}{1127} \\\\\n",
    "    P(S|\\text{renew your password}) &= 0.0026619343389529724 \\\\\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "Where \"renew\" is unknown in the Spam corpus and smoothed to $\\frac{1}{14}$. And \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(H|\\text{renew your password}) &= P(H) \\times P(renew | H) \\times P(your | H \\times P(password | H) \\\\\n",
    "    P(H|\\text{renew your password}) &= \\frac{9}{23} \\times \\frac{1}{9}   \\times  \\frac{1}{9} \\times \\frac{1}{9}  \\\\\n",
    "    P(H|\\text{renew your password}) &= \\frac{1}{1863} \\\\\n",
    "    P(H|\\text{renew your password}) &= 0.0005367686527106816 \\\\\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "Where \"renew\" is unknown in the ham corpus and smoothed to $\\frac{1}{9}$\n",
    "\n",
    "If $P(S|\\text{renew your password}) > P(H|\\text{renew your password})$ then SPAM, else HAM. In this case, `renew your password` is predicted to be `spam`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's package all of the previous code into classes, so it's more modular. Note, I will do some refactoring but keep the same behaviour as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTester:\n",
    "    \"\"\" \n",
    "    The prupose of this class is to test the classifier. If I was doing \n",
    "    this properly, I would use a notion of abstract methods so the\n",
    "    tester could test the accuracy of any approach that uses the same \n",
    "    signatures for fit and predict.\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def test(self, test_data, print_results=True):\n",
    "        \"\"\"\n",
    "        Test the classifier with test data. This just measures\n",
    "        how many correct predictions the classifier makes (divided\n",
    "        by the total number of possible predictions).     \n",
    "\n",
    "        :param test_data: a dictionary of test data, where \n",
    "                          the key is the label and the value \n",
    "                          is a list of emails\n",
    "        :param print_results: whether to print the results \n",
    "                          (highlight mismatches)\n",
    "        :return: the accuracy of the classifier\n",
    "        \"\"\"\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "\n",
    "        max_len = max([len(email) for emails in test_data.values() for email in emails])\n",
    "\n",
    "        if print_results:\n",
    "            print(f'\\t{\"Message\":{max_len}} {\"Expected\":10} {\"Predicted\":10}')\n",
    "            print(('\\t'*1 + '-' * max_len) + ' ' + '-' * 10 + ' ' + '-' * 10)\n",
    "\n",
    "        for label, emails in test_data.items():\n",
    "            for email in emails:\n",
    "                predicted_label = self.classifier.predict(email)\n",
    "\n",
    "                if print_results:\n",
    "                    print(f'\\t{email:{max_len}} {label:10} {predicted_label:10} {\"WRONG!\" if predicted_label != label else \"CORRECT!\"}')\n",
    "\n",
    "                if predicted_label == label:\n",
    "                    num_correct += 1\n",
    "                num_total += 1\n",
    "\n",
    "        accuracy = num_correct / num_total\n",
    "        \n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the NaiveBayesClassifier.\n",
    "        \"\"\"\n",
    "        self._spam_list = []\n",
    "        self._ham_list = []\n",
    "        self._vocabulary = set()\n",
    "        self._alpha = 1  # Laplace smoothing parameter\n",
    "        self._spam_word_counts = {}\n",
    "        self._ham_word_counts = {}\n",
    "        self._num_spam_words = 0\n",
    "        self._num_ham_words = 0\n",
    "        self._prior_spam = 0\n",
    "        self._prior_ham = 0\n",
    "        self._spam_likelihoods = {}\n",
    "        self._ham_likelihoods = {}\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        This just helps pretty print the class to help understand the internals a little better.\n",
    "\n",
    "        :return: a string representation of the classifier.\n",
    "        \"\"\"\n",
    "        return (f'NB Classifier with {len(self._vocabulary)} words.\\n'\n",
    "                f'\\tA priori probabilities: spam={self._prior_spam:.4f}, ham={self._prior_ham:.4f}.\\n'\n",
    "                f'\\tAlpha parameter: {self._alpha}')\n",
    "\n",
    "    def fit(self, spam_list: list, ham_list: list):\n",
    "        \"\"\"\n",
    "        Train the classifier with the provided spam and ham lists.\n",
    "        \n",
    "        :param spam_list: List of spam sentences.\n",
    "        :param ham_list: List of ham sentences.\n",
    "        \"\"\"\n",
    "        self._spam_list = spam_list\n",
    "        self._ham_list = ham_list\n",
    "\n",
    "        self._vocabulary = self._build_vocabulary()\n",
    "        self._spam_word_counts, self._ham_word_counts = self._build_word_counts()\n",
    "        self._num_spam_words = sum(self._spam_word_counts.values())\n",
    "        self._num_ham_words = sum(self._ham_word_counts.values())\n",
    "        self._prior_spam = self._num_spam_words / (self._num_spam_words + self._num_ham_words)\n",
    "        self._prior_ham = 1 - self._prior_spam\n",
    "        self._spam_likelihoods = self._compute_likelihoods(self._spam_word_counts, self._num_spam_words)\n",
    "        self._ham_likelihoods = self._compute_likelihoods(self._ham_word_counts, self._num_ham_words)\n",
    "\n",
    "    def _build_vocabulary(self) -> set:\n",
    "        \"\"\"\n",
    "        Build a vocabulary from the spam and ham lists.\n",
    "\n",
    "        :return: The vocabulary as a set (unique words).\n",
    "        \"\"\"\n",
    "        vocabulary = set()\n",
    "        for email in self._spam_list + self._ham_list:\n",
    "            words = email.split()\n",
    "            for word in words:\n",
    "                vocabulary.add(word)\n",
    "        return vocabulary\n",
    "\n",
    "    def _build_word_counts(self) -> (dict, dict):\n",
    "        \"\"\"\n",
    "        Build dictionaries of word counts for spam and ham.\n",
    "\n",
    "        :return: Dictionaries of word counts.\n",
    "        \"\"\"\n",
    "        spam_word_counts = {word: 0 for word in self._vocabulary}\n",
    "        ham_word_counts = {word: 0 for word in self._vocabulary}\n",
    "\n",
    "        for email in self._spam_list:\n",
    "            for word in email.split():\n",
    "                if word in spam_word_counts:\n",
    "                    spam_word_counts[word] += 1\n",
    "\n",
    "        for email in self._ham_list:\n",
    "            for word in email.split():\n",
    "                if word in ham_word_counts:\n",
    "                    ham_word_counts[word] += 1\n",
    "\n",
    "        return spam_word_counts, ham_word_counts\n",
    "\n",
    "    def _compute_likelihoods(self, word_counts: dict, total_words: int) -> dict:\n",
    "        \"\"\"\n",
    "        Compute the likelihood of each word.\n",
    "\n",
    "        :param word_counts: The frequency of each word.\n",
    "        :param total_words: Total number of words.\n",
    "        :return: Dictionary of likelihoods.\n",
    "        \"\"\"\n",
    "        return {word: (count + self._alpha) / total_words for word, count in word_counts.items()}\n",
    "\n",
    "    def update(self, new_spam: list, new_ham: list):\n",
    "        \"\"\"\n",
    "        Update the classifier with new spam and ham messages.\n",
    "\n",
    "        :param new_spam: List of new spam messages.\n",
    "        :param new_ham: List of new ham messages.\n",
    "        \"\"\"\n",
    "        self._spam_list.extend(new_spam)\n",
    "        self._ham_list.extend(new_ham)\n",
    "\n",
    "        # Now refit for priors and likelihoods\n",
    "        self.fit(self._spam_list, self._ham_list)\n",
    "\n",
    "    def predict(self, msg: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify a single email as spam or ham.\n",
    "\n",
    "        :param msg: The email message to classify.\n",
    "        :return: Classification as 'SPAM' or 'HAM'.\n",
    "        \"\"\"\n",
    "        words = msg.split()\n",
    "\n",
    "        spam_posterior_prob = self._prior_spam\n",
    "        ham_posterior_prob = self._prior_ham\n",
    "\n",
    "        for word in words:\n",
    "            spam_posterior_prob *= self._spam_likelihoods.get(word, self._alpha / self._num_spam_words)\n",
    "            ham_posterior_prob *= self._ham_likelihoods.get(word, self._alpha / self._num_ham_words)\n",
    "\n",
    "        return 'spam' if spam_posterior_prob > ham_posterior_prob else 'ham'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier with 16 words.\n",
      "\tA priori probabilities: spam=0.6087, ham=0.3913.\n",
      "\tAlpha parameter: 1\n",
      "\n",
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\trenew your password                 spam       spam       CORRECT!\n",
      "\trenew your vows                     spam       ham        WRONG!\n",
      "\tbenefits of our account             ham        ham        CORRECT!\n",
      "\tthe importance of physical activity ham        ham        CORRECT!\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "print(f\"{nb_classifier}\\n\")\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "#How good is the classifier?\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I demonstrate a super-naive way to update an existing model and re-test. Again, there;s no notion of serialisation/deserialisation so if the classifier goes out of scope, you have to rebuild everything. there really are better ways to do this... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction for \"Give me your password\": spam\n",
      "Prediction for \"Send me your data\": spam\n",
      "Prediction for \"Give me your money\": spam\n",
      "Prediction for \"I would love a car\": ham\n"
     ]
    }
   ],
   "source": [
    "# Update the classifier\n",
    "nb_classifier.update(new_emails['spam'], new_emails['ham'])\n",
    "\n",
    "# Present new data to the classifier\n",
    "new_msg = \"Give me your password\"\n",
    "result = nb_classifier.predict(new_msg)\n",
    "\n",
    "print(f'\\nPrediction for \\\"{new_msg}\\\": {result}')\n",
    "\n",
    "new_msg = \"Send me your data\"\n",
    "result = nb_classifier.predict(new_msg)\n",
    "\n",
    "print(f'Prediction for \\\"{new_msg}\\\": {result}')\n",
    "\n",
    "new_msg = \"Give me your money\"\n",
    "result = nb_classifier.predict(new_msg)\n",
    "\n",
    "print(f'Prediction for \\\"{new_msg}\\\": {result}')\n",
    "\n",
    "new_msg = \"I would love a car\"\n",
    "result = nb_classifier.predict(new_msg)\n",
    "\n",
    "print(f'Prediction for \\\"{new_msg}\\\": {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This toy NB bayes approach demonstrates the use or prior knowledge of SPAM/HAM in the classification of new messages. We would constantly update the likehoods as we get new corrected labelled data.\n",
    "\n",
    "### Sources\n",
    "I used all of the references below to build this classifier.\n",
    "\n",
    "### Feedback\n",
    "`PENDING`\n",
    "\n",
    "### System Design\n",
    "The NaiveBayesClassifier class allows:\n",
    "\n",
    "* Training an NB classifier. Training (or fitting) generates the priors and likelihoods for the data set.\n",
    "* Updates the priors and likelihoods based on new, labelled data.\n",
    "\n",
    "The ClassifierTester class enables us to establish the accuracy of the classifier using labelled, but previously, unseen data.\n",
    "\n",
    "#### Advantages \n",
    "* Surprisingly good even if it assumes all words are independant (which is mostly, clearly not the case in language).\n",
    "* Easy to understand and \n",
    "* relatively quick to implement\n",
    "\n",
    "#### Disadvantages\n",
    "* The NB classifier has no sense of order (or context).\n",
    "* The data set is far to small.\n",
    "* we assume all input is lowercase. There's no facility for stopwords.\n",
    "* The system has no notion of punctuation or spelling mistakes.\n",
    "* The current design calls for the NB class to retain the labelled data that it has previously seen. This is poor as, once the object goes out of scope, the GC will clean it up and all training data will be lost. Ideally, we would at the very least, save the core data to disk and deserialise when we reinstantiate an opbject.\n",
    "\n",
    "### Discussion\n",
    "#### Using a Threshold\n",
    "I have included two ways to predict whether a message is SPAM or HAM:\n",
    "\n",
    "* Comparingthe relative posterior probabilities (picking the highest as the most likely)\n",
    "* Using a threshold\n",
    "\n",
    "I prefer the former asthe latter approach raises the question of what threshold should I set; i.e. how good is good enough.\n",
    "\n",
    "#### Text Pre-processing: Stopwords\n",
    "Removing stop words actually drops the accuracy down to 50/50. Toolkits like ntlk have modules with many stops works (for english and many other languages). In this case, with so few words, removing stops words may actually remove information from the train and test sets.\n",
    "\n",
    "#### Text Pre-processing: Remove punctuation, force lower case\n",
    "We could pre-process the test to remove punctuation and force all text to be lowercase. \n",
    "\n",
    "#### Accuracy (with different test/train splits or with new data)\n",
    "Note that the accuracy of the system can be affected by changing how we divide the training and tests set. See here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\tsend us your password               spam       spam       CORRECT!\n",
      "\trenew your password                 spam       spam       CORRECT!\n",
      "\tbenefits physical activity          ham        ham        CORRECT!\n",
      "\tthe importance of physical activity ham        ham        CORRECT!\n",
      "\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# take a SPAM message from previous and move to test and vice versa.\n",
    "previous_spam = ['renew your vows', 'review our website', 'send your password', 'send us your account']\n",
    "previous_ham = ['benefits of our account','Your activity report', 'the importance vows']\n",
    "new_emails = {'spam':['send us your password', 'renew your password' ], 'ham':['benefits physical activity', 'the importance of physical activity']}\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i get an accuracy of 1.0??? Very suspect! The same labelled messages exist in both test and train sets, I've just changed what I used to test and train with; and I get a different accuracy depending on the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, increasing the number of samples in the test set will drive the accuracy down (see the example below where I have added one new message to the test set - taking the accuracy from 0.75 to 0.6). This is shown next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\trenew your password                 spam       spam       CORRECT!\n",
      "\trenew your vows                     spam       ham        WRONG!\n",
      "\tbenefits of our account             ham        ham        CORRECT!\n",
      "\tyour exam results are in            ham        ham        CORRECT!\n",
      "\tthe importance of physical activity ham        ham        CORRECT!\n",
      "\n",
      "Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "previous_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
    "previous_ham = ['Your activity report','benefits physical activity', 'the importance vows']\n",
    "new_emails = {'spam':['renew your password', 'renew your vows'], 'ham':['benefits of our account', 'your exam results are in', 'the importance of physical activity']}\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### False Positives\n",
    "A few years ago, I worked in the AV industry as a malware analyst and we always had to deal with notions of false positive results (TP, TN, FP, FN); think how many times your AV thought a valid, reputable application was malware. Misclassification is a fact of life with these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bags of Words and (Not) Understanding Context\n",
    "Note that this approach is naive in many ways. It assumes the features are independance. Consider what happens if we jumble up the words in our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\tyour renew  password                spam       spam       CORRECT!\n",
      "\trenew vows your                     spam       ham        WRONG!\n",
      "\tour benefits of account             ham        ham        CORRECT!\n",
      "\tthe importance of physical activity ham        ham        CORRECT!\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "previous_spam = ['us send password your', 'website our review', 'your password send', 'send us your account']\n",
    "previous_ham = ['activity report Your', 'benefits activity physical', 'the vows importance']\n",
    "new_emails = {'spam': ['your renew  password', 'renew vows your'], 'ham': ['our benefits of account', 'the importance of physical activity']}\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result. This is not an exhaustive test but demonstrates the intuition that the method ignores word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating floating point underflow\n",
    "Since we cannot import libraries, we will need to approximate the log function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Naive Bayes, Clearly Explain!!!](https://www.youtube.com/watch?v=O2L2Uv9pdDA&ab_channel=StatQuestwithJoshStarmer)\n",
    "* [Bayes theorem, the geometry of changing beliefs](https://www.youtube.com/watch?v=HZGCoVF3YvM&ab_channel=3Blue1Brown)\n",
    "* [Notes on Naive Bayes Classifiers for Spam Filtering](https://courses.cs.washington.edu/courses/cse312/18sp/lectures/naive-bayes/naivebayesnotes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
