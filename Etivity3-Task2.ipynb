{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook E-tivity 3 CE4021 Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name:** Jason Coleman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID:** 9539719"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you believe required imports are missing, please contact your moderator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below information to create a Naive Bayes SPAM filter. Test your filter using the messages in new_emails. You may add as many cells as you require to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
    "previous_ham = ['Your activity report','benefits physical activity', 'the importance vows']\n",
    "new_emails = {'spam':['RENEW your password', 'renew your vows'], 'ham':['benefits of our account', 'the importance of physical activity']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This will be a very naive Naive Bayes classifier. It will follow a so-called `Bag-of-words` approach where I will classify a message as `SPAM` or `HAM` based on just the presence of the words in the string. Notably, the bag of words does not consider the context in which the work is used (i.e. you can reorder the words into complete nonsense and get the same result; assuming the train and test set split is maintained). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Bayes Theorem\n",
    "Let us start with the formula for Bayes Theorem. \n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "* where $P(A \\mid B)$ is the probability of event A happening given that event B has happened.\n",
    "* and $P(B \\mid A)$ is the probability of event B given A.\n",
    "* and $P(A)$ and $P(B)$ are the probabilities of events A and B, respectively.\n",
    "\n",
    "In the context of our spam filter:\n",
    "\n",
    "\n",
    "*  where $A$ could be the event \"The email is spam\".\n",
    "*  and $B$ could be \"The email contains the word 'pAssword'\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier (NB)\n",
    "\n",
    "NB is a classification technique based on Bayes' theorem; with the \"naive\" assumption that every pair of features is independent of each other. Even though it is called naive, it can perform surprisingly well. \n",
    "\n",
    "**Aside:** I worked as an analyst in the AV industry a few years ago and, in the early days, when we had less data (and features) NB worked really well (this was before the recent, crazy advanced in CNN, LSTM and transformers). \n",
    "\n",
    "We can reframe our use of Bayes a little, in the context of SPAM/HAM.\n",
    "\n",
    "* Where $S$ represents the event that an email is spam.\n",
    "* and $W$ represents the event that a specific word appears in the email.\n",
    "\n",
    "We can write:\n",
    "\n",
    "$$\n",
    "P(S \\mid W) = \\frac{P(W \\mid S) \\times P(S)}{P(W)}\n",
    "$$\n",
    "\n",
    "\n",
    "*  Where $P(S \\mid W)$ is the probability that an email is spam given that it contains the word $W$.\n",
    "*  and $P(W \\mid S)$ is the probability that the word $W$ appears in a spam email.\n",
    "*  and $P(S)$ is the overall probability (or prior) that any email is spam. This is a `prior`. \n",
    "*  and $P(W)$ is the probability that the word $W$ appears in any email, regardless of it being spam or not. This is a `prior`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "Then, given a new message, if $P(S | m) > P(H | m)$, I will classify the message as SPAM, else I will classify the message as Ham. \n",
    "\n",
    "By doing this, I am side-stepping using a threshold and just comparing the relative posterior probabilitites for the message being HAM and SPAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "We will need to preprocess the data a little; to get it into the right shape. We will start by getting the components of the Bayes formula: the `priors` and the `likelihoods` for the words within the messages. \n",
    "\n",
    "I will use a process called **Tokenising:**, the breaking of each message into words. Each word is called a `token`. It's the fundemental unit of classification in this Bag of Words classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "* I will reference the steps in the reference implementation\n",
    "* My implementation will use list and dictionary comprehensions to make the code more concise.\n",
    "* I will print intermediate output to aid understanding and debugging. This may make code a little harder to read. \n",
    "*  I will pre-process text to make it all lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all words in 2 lists lower case\n",
    "previous_spam = [x.lower() for x in previous_spam]\n",
    "previous_ham = [x.lower() for x in previous_ham]\n",
    "new_emails['spam'] = [x.lower() for x in new_emails['spam']]\n",
    "new_emails['ham'] = [x.lower() for x in new_emails['ham']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working on messages with more than one word\n",
    "\n",
    "Emails are rarely composed of just one word so our implementation will need to account for all of the words in the message. For a given email message, $m$, consisting of words $w_1$, $w_2$, $\\ldots$, $w_n$, we need to modify the method to account for multiple words. \n",
    "\n",
    "0. We will need to calculate the likelihoods for words being both SPAM or HAM (i.e. the probabiity of a word appearing in SPAM). That is: $P(w_1 | S)$, $P(w_1 | H)$ for each word int he training data. \n",
    "\n",
    "Then, for the training messages:\n",
    "\n",
    "1. Calculate the likelihood of the email being Spam, based on the likelihoods of all words in the message:\n",
    "\n",
    "$$\n",
    "P(m | S) = P(w_1 | S) \\times P(w_2 | S) \\times \\ldots \\times P(w_n | S)\n",
    "$$\n",
    "\n",
    "2. Calculate the likelihood of the email being Ham, based on word frequncy:\n",
    "\n",
    "$$\n",
    "P(m | H) = P(w_1 | H) \\times P(w_2 | H) \\times \\ldots \\times P(w_n | H)\n",
    "$$\n",
    "\n",
    "Then, finally, calculate the posterior probabilitites for being Ham and Spam.\n",
    "\n",
    "$$\n",
    "P(S | m) = \\frac{P(m | S) \\times P(S)}{P(m)}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "P(H | m) = \\frac{P(m | H) \\times P(H)}{P(m)}\n",
    "$$\n",
    "\n",
    "* Where $P(S)$ and $P(H)$ are the prior probabilities of a message being spam and ham, respectively (we know this from the initial training set).\n",
    "* and $P(m)$ is the total probability of observing message $m$. \n",
    "\n",
    "Note, I don't need to calculate $P(m)$ if I am comparing the relative probability of $P(S | m)$ and $P(H | m)$ so the method I will implement will actually be the following:\n",
    "\n",
    "$$\n",
    "P(S | m) = P(w_1 | S) \\times P(w_2 | S) \\times \\ldots \\times P(w_n | S) \\times P(S)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "P(H | m) = P(w_1 | H) \\times P(w_2 | H) \\times \\ldots \\times P(w_n | H)\\times P(H)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary\n",
    "We will need to calculate Priors and likelihoods. To do this we first need to construct a vocabulary. The prior probability of an email being spam or ham is calculated based on the proportion of spam or ham emails in the training data.\n",
    "\n",
    "1. Create a set called *vocabulary*.\n",
    "2. For each email in the SPAM and HAM training messages, split the words\n",
    "3. Add all words to the set.\n",
    "\n",
    "We now have a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: us, benefits, send, our, website, the, your, report, physical, importance, account, review, password, vows, activity, "
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for email in previous_spam + previous_ham:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        vocabulary.add(word)\n",
    "\n",
    "print(\"Vocabulary:\", end=\" \")\n",
    "for word in vocabulary:\n",
    "    print(word, end=\", \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probability of an email being spam or ham is calculated based on the ocurrence of those words in both the spam and ham training sets.\n",
    "\n",
    "$$\n",
    "\\texttt{prior\\_spam} = \\frac{\\texttt{num\\_spam\\_words}}{\\texttt{total\\_words}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\texttt{prior\\_ham} = \\frac{\\texttt{num\\_ham\\_words}}{\\texttt{total\\_words}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14 spam words in the training set.\n",
      "There are 9 ham words in the training set.\n",
      "\n",
      "All words: send, us, your, password, review, our, website, send, your, password, send, us, your, account, your, activity, report, benefits, physical, activity, the, importance, vows, Total words in the training set: 23\n",
      "Prior Spam: 0.6086956521739131\n",
      "Prior Ham: 0.3913043478260869\n"
     ]
    }
   ],
   "source": [
    "#get words from previous_spam\n",
    "spam_words = []\n",
    "for email in previous_spam:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        spam_words.append(word)\n",
    "\n",
    "num_spam_words = len(spam_words)\n",
    "\n",
    "print (f\"There are {len(spam_words)} spam words in the training set.\")\n",
    "        \n",
    "# get words from previous_ham\n",
    "ham_words = []\n",
    "for email in previous_ham:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        ham_words.append(word)\n",
    "\n",
    "num_ham_words = len(ham_words)\n",
    "\n",
    "print (f\"There are {len(ham_words)} ham words in the training set.\")\n",
    "print(\"\\nAll words:\", end=\" \")\n",
    "for word in spam_words + ham_words:\n",
    "    print(word, end=\", \")\n",
    "\n",
    "total_words = len(spam_words) + len(ham_words) # total number of words in spam emails\n",
    "print( f\"Total words in the training set: {total_words}\")\n",
    "\n",
    "prior_spam = num_spam_words / (num_spam_words + num_ham_words)\n",
    "prior_ham = 1 - prior_spam\n",
    "\n",
    "print(f\"Prior Spam: {prior_spam}\")\n",
    "print(f\"Prior Ham: {prior_ham}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Conditional Probabilitites (aka Likelihoods)\n",
    "Now, for each word in the vocabulary, I determine the likelihood that the word exists in messages labelled as both SPAM and HAM. I do this by:\n",
    "\n",
    "1. Counting the occurrences of each word in both the spam and ham emails.\n",
    "2. Apply Laplace Smoothing to avoid zero probabilities. If I don't add this, I will eventually end up multiplying by zero (as some words have never been seen before, the conditional probability for hat word will be zero).\n",
    "3. Calculate the conditional probabilities (likelihoods) for each word being both SPAM and HAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      "{'us', 'benefits', 'send', 'our', 'website', 'the', 'your', 'report', 'physical', 'importance', 'account', 'review', 'password', 'vows', 'activity'}\n",
      "\n",
      "Spam/HAM dictionaries are as follows:\n",
      "\n",
      "\tWords\t\tSpam Count \tHam Count\n",
      "\t----------------------------------------\n",
      "\tus        \t 2\t\t 0\n",
      "\tbenefits  \t 0\t\t 1\n",
      "\tsend      \t 3\t\t 0\n",
      "\tour       \t 1\t\t 0\n",
      "\twebsite   \t 1\t\t 0\n",
      "\tthe       \t 0\t\t 1\n",
      "\tyour      \t 3\t\t 1\n",
      "\treport    \t 0\t\t 1\n",
      "\tphysical  \t 0\t\t 1\n",
      "\timportance\t 0\t\t 1\n",
      "\taccount   \t 1\t\t 0\n",
      "\treview    \t 1\t\t 0\n",
      "\tpassword  \t 2\t\t 0\n",
      "\tvows      \t 0\t\t 1\n",
      "\tactivity  \t 0\t\t 2\n",
      "\n",
      "The condtional probabilities (likelihoods) for spam and ham are as follows (.md format):\n",
      "\n",
      "|Words\t\t|P(w\\|S)\t|P(w\\|H)|\n",
      "|------------|--------------|-------------|\n",
      "|us        \t|0.2143|\t\t0.1111|\n",
      "|benefits  \t|0.0714|\t\t0.2222|\n",
      "|send      \t|0.2857|\t\t0.1111|\n",
      "|our       \t|0.1429|\t\t0.1111|\n",
      "|website   \t|0.1429|\t\t0.1111|\n",
      "|the       \t|0.0714|\t\t0.2222|\n",
      "|your      \t|0.2857|\t\t0.2222|\n",
      "|report    \t|0.0714|\t\t0.2222|\n",
      "|physical  \t|0.0714|\t\t0.2222|\n",
      "|importance\t|0.0714|\t\t0.2222|\n",
      "|account   \t|0.1429|\t\t0.1111|\n",
      "|review    \t|0.1429|\t\t0.1111|\n",
      "|password  \t|0.2143|\t\t0.1111|\n",
      "|vows      \t|0.0714|\t\t0.2222|\n",
      "|activity  \t|0.0714|\t\t0.3333|\n"
     ]
    }
   ],
   "source": [
    "# Create dictionaries from the vocabulary to store word counts, initialise the count to zero\n",
    "spam_word_counts = {word: 0 for word in vocabulary}\n",
    "ham_word_counts = {word: 0 for word in vocabulary}\n",
    "print(f\"Vocabulary: \\n{vocabulary}\\n\")\n",
    "\n",
    "# Determine frequencies of words in spam and ham emails (I cosider repeatred words here)\n",
    "for email in previous_spam:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        spam_word_counts[word] += 1\n",
    "\n",
    "for email in previous_ham:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        ham_word_counts[word] += 1\n",
    "\n",
    "print(\"Spam/HAM dictionaries are as follows:\\n\")\n",
    "\n",
    "# print the dictionaries for spam ham in one table\n",
    "print(\"\\tWords\\t\\tSpam Count \\tHam Count\")\n",
    "print(\"\\t\"+\"-\" * 40)\n",
    "for word in vocabulary:\n",
    "    print(f\"\\t{word:10s}\\t{spam_word_counts[word]:2d}\\t\\t{ham_word_counts[word]:2d}\")\n",
    "\n",
    "print()\n",
    "\n",
    "#  Preparing the (smoothed) likelihoods\n",
    "alpha = 1 # Laplace smoothing (in this case 'add-one' smoothing) parameter\n",
    "\n",
    "# Compute the likeloods for SPAM/HAM.\n",
    "spam_likelihoods = {word: (count + alpha) / num_spam_words for word, count in spam_word_counts.items()}\n",
    "ham_likelihoods = {word: (count + alpha) / num_ham_words for word, count in ham_word_counts.items()}\n",
    "\n",
    "print(\"The condtional probabilities (likelihoods) for spam and ham are as follows (.md format):\\n\")\n",
    "\n",
    "# print a table of spam_probs and ham_probs\n",
    "print(\"|Words\\t\\t|P(w\\|S)\\t|P(w\\|H)|\")\n",
    "print(\"|------------|--------------|-------------|\")\n",
    "for word in vocabulary:\n",
    "    print(f\"|{word:10s}\\t|{spam_likelihoods[word]:.4f}|\\t\\t{ham_likelihoods[word]:.4f}|\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Classifier\n",
    "I can now test the classfier on the test set of data. I will use the previously calculated priors and likelihood to calculate the posterior probabilities for both SPAM and HAM and this will allow me to classify new emails as spam or ham. The classification is done by comparing the posterior probability for Ham to that of SPAM: the highest one is the output label.\n",
    "\n",
    "I can assess the accuracy of the classifier against data it has not seen before. We do this with a labelled test set (just count how many predictions we got right and divide by the total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SPAM msg: \"renew your password\"\n",
      "\tSpam Posterior Probability: 0.0026619343389529724\n",
      "\tHam Posterior Probability: 0.0010735373054213632\n",
      "renew your password            spam  CORRECT!\n",
      "\n",
      "SPAM msg: \"renew your vows\"\n",
      "\tSpam Posterior Probability: 0.0008873114463176574\n",
      "\tHam Posterior Probability: 0.0021470746108427263\n",
      "renew your vows                ham   WRONG!\n",
      "\n",
      "HAM msg: \"benefits of our account\"\n",
      "\tSpam Posterior Probability: 6.337938902268981e-05\n",
      "\tHam Posterior Probability: 0.0001192819228245959\n",
      "benefits of our account        ham   CORRECT!\n",
      "\n",
      "HAM msg: \"the importance of physical activity\"\n",
      "\tSpam Posterior Probability: 1.1317748039766037e-06\n",
      "\tHam Posterior Probability: 0.00015904256376612786\n",
      "the importance of physical activity ham   CORRECT!\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_total = 0\n",
    "\n",
    "# for all labelled messages in the test set, grouped by label\n",
    "for expected_label, msgs in new_emails.items():\n",
    "    for msg in msgs:\n",
    "\n",
    "        print(f\"\\n{expected_label.upper()} msg: \\\"{msg}\\\"\")\n",
    "\n",
    "        words = msg.split() # remember: tokenise\n",
    "\n",
    "        # initialise with the priors\n",
    "        spam_posterior_prob = prior_spam\n",
    "        ham_posterior_prob = prior_ham\n",
    " \n",
    "        # Calculate the posterior probabilities based on the multiplied likelihoods\n",
    "        for word in words:\n",
    "            if word in spam_likelihoods:\n",
    "                spam_posterior_prob *= spam_likelihoods[word]\n",
    "            else: # if the word is new, use smoothing\n",
    "                spam_posterior_prob *= alpha / num_spam_words \n",
    "                \n",
    "            if word in ham_likelihoods:\n",
    "                ham_posterior_prob *= ham_likelihoods[word]\n",
    "            else: # if the word is new, use smoothing\n",
    "                ham_posterior_prob *= alpha / num_ham_words\n",
    "\n",
    "        print(f\"\\tSpam Posterior Probability: {spam_posterior_prob}\")\n",
    "        print(f\"\\tHam Posterior Probability: {ham_posterior_prob}\")\n",
    "                \n",
    "        # I think it is better to compare the relative probabilities instead of\n",
    "        # picking a threshold. You always get into a trade-off between false\n",
    "        # positives and false negatives.        \n",
    "        if spam_posterior_prob > ham_posterior_prob:\n",
    "            predicted_label = \"spam\"\n",
    "        else:\n",
    "            predicted_label = \"ham\"\n",
    "\n",
    "        status = \"WRONG!\" if predicted_label != expected_label else \"CORRECT!\"\n",
    "        print(f\"{msg:30s} {predicted_label:5s} {status}\")\n",
    "        \n",
    "        if predicted_label == expected_label:\n",
    "            num_correct += 1\n",
    "\n",
    "        num_total += 1\n",
    "\n",
    "accuracy = num_correct / num_total\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this all work?\n",
    "The approach relies on Bayes' Rule, which relates the probability of a message being SPAM based on prior knowledge (that is, having seen examples of HAM and SPAM) and the occurrence of component words in the two classes of training data (the classes are Spam and Ham). \n",
    "\n",
    "The approach taken relies on pre-processing the data to ensure I have a vocabulary of words and an understanding of their occurrence in terms of being SPAM or HAM (i.e. P(w|S) and P(w|H)). \n",
    "\n",
    "There are two main stages to this process: `fitting` and `prediction`.\n",
    "\n",
    "**Fitting:**\n",
    "\n",
    "* **Compute Priors:** Calculate the prior probabilities, based on the frequency of spam and ham emails in the training data.\n",
    "* **Compute Likelihoods:** For each word in our vocabulary, compute the likelihood it appears in spam and ham emails. Note, as directed, that I use Laplace smoothing to make sure that any words not seen in the training set don't zero out the probabilities when making predictions (i.e. multiplying by zero pulls the probability down to zero - this is called Laplacian smoothing and is described in the branch).\n",
    "\n",
    "In our case, for our training data set the Priors are calculated as:\n",
    "\n",
    "$$\n",
    "P(S) = \\frac{14}{23} = 0.6086956521739131\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(H) = \\frac{9}{23} = 0.3913043478260869\n",
    "$$\n",
    "\n",
    "The conditional probabilities (or likelihoods) are calculated to be:\n",
    "\n",
    "\n",
    "| Words       | P(w\\|S)  | Fractional P(w\\|S) | P(w\\|H) | Fractional P(w\\|H) |\n",
    "|-------------|---------|--------------------|---------|--------------------|\n",
    "| us          | 0.2143  | $3/14$             | 0.1111  | $1/9$              |\n",
    "| the         | 0.0714  | $1/14$             | 0.2222  | $2/9$              |\n",
    "| password    | 0.2143  | $3/14$             | 0.1111  | $1/9$              |\n",
    "| importance  | 0.0714  | $1/14$             | 0.2222  | $2/9$              |\n",
    "| send        | 0.2857  | $2/7$              | 0.1111  | $1/9$              |\n",
    "| review      | 0.1429  | $1/7$              | 0.1111  | $1/9$              |\n",
    "| your        | 0.2857  | $2/7$              | 0.2222  | $2/9$              |\n",
    "| report      | 0.0714  | $1/14$             | 0.2222  | $2/9$              |\n",
    "| vows        | 0.0714  | $1/14$             | 0.2222  | $2/9$              |\n",
    "| account     | 0.1429  | $1/7$              | 0.1111  | $1/9$              |\n",
    "| physical    | 0.0714  | $1/14$             | 0.2222  | $2/9$              |\n",
    "| website     | 0.1429  | $1/7$              | 0.1111  | $1/9$              |\n",
    "| benefits    | 0.0714  | $1/14$             | 0.2222  | $2/9$              |\n",
    "| our         | 0.1429  | $1/7$              | 0.1111  | $1/9$              |\n",
    "| activity    | 0.0714  | $1/14$             | 0.3333  | $1/3$              |\n",
    "\n",
    "\n",
    "When calculating posterior probabiltities for spam, unknown words will be assigned, $\\frac{1}{14}$; and for ham, unknown words will be assigned $\\frac{1}{9}$. \n",
    "\n",
    "**Prediction:**\n",
    "\n",
    "* For a new, unlabeled email, break it down into its constituent words (all lower case, with zero punctuation - which is itself information).\n",
    "* For each word, multiply the likelihoods, initialising the product with the prior (I do two passes: one to calculate the conditional probabilities for SPAM and one for HAM). \n",
    "* **Apply Bayes Rule**. Lastly, I combine the prior probabilities with the computed likelihoods to get the posterior probabilities that the email is spam or ham; classifying the email based on whichever probability is higher, SPAM or HAM.\n",
    "\n",
    "I then evaluate the accuracy of the model by predicting the labels of a test set of data and measuring how many we predict correctly. \n",
    "\n",
    "Note, you can always re-fit the models as you get more data, improving the system's awareness of what is (and is not) SPAM (see the update method on the python class).\n",
    "\n",
    "### Step-by-step: Classify (or Predict the Class) for the message: \"renew your password\"\n",
    "\n",
    "As an example, I take the test message, `\"renew your password\"` and compute the following:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(S|\\text{renew your password}) &= P(S)          \\times P(renew | S) \\times P(your | S)   \\times P(password | S) \\\\\n",
    "    P(S|\\text{renew your password}) &= \\frac{14}{23} \\times \\frac{2}{7}  \\times  \\frac{1}{14} \\times \\frac{3}{14}  \\\\\n",
    "    P(S|\\text{renew your password}) &= \\frac{3}{1127} \\\\\n",
    "    P(S|\\text{renew your password}) &= 0.0026619343389529724 \\\\\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "Where \"renew\" is unknown in the Spam corpus and smoothed to $\\frac{2}{7}$. And \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    P(H|\\text{renew your password}) &= P(H)        \\times P(renew | H)   \\times P(your | H   \\times P(password | H) \\\\\n",
    "    P(H|\\text{renew your password}) &= \\frac{9}{23} \\times \\frac{2}{9}   \\times  \\frac{1}{9} \\times \\frac{1}{9}  \\\\\n",
    "    P(H|\\text{renew your password}) &= \\frac{1}{1863} \\\\\n",
    "    P(H|\\text{renew your password}) &= 0.0005367686527106816 \\\\\n",
    "\\end{align*} \n",
    "$$\n",
    "\n",
    "Where \"renew\" is unknown in the ham corpus and smoothed to $\\frac{2}{9}$\n",
    "\n",
    "If $P(S|\\text{renew your password}) > P(H|\\text{renew your password})$ then SPAM, else HAM. In this case, `renew your password` is predicted to be `spam`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class for the Naive Bayes Classifier\n",
    "Let's package all of the previous code into classes, so it's more modular. Note, I will do some refactoring but keep the same behaviour as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTester:\n",
    "    \"\"\" \n",
    "    The prupose of this class is to test the classifier. If I was doing \n",
    "    this properly, I would use a notion of abstract methods so the\n",
    "    tester could test the accuracy of any approach that uses the same \n",
    "    signatures for fit and predict.\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def test(self, test_data, print_results=True):\n",
    "        \"\"\"\n",
    "        Test the classifier with test data. This just measures\n",
    "        how many correct predictions the classifier makes (divided\n",
    "        by the total number of possible predictions).     \n",
    "\n",
    "        :param test_data: a dictionary of test data, where \n",
    "                          the key is the label and the value \n",
    "                          is a list of emails\n",
    "        :param print_results: whether to print the results \n",
    "                          (highlight mismatches)\n",
    "        :return: the accuracy of the classifier\n",
    "        \"\"\"\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "\n",
    "        max_len = max([len(email) for emails in test_data.values() for email in emails])\n",
    "\n",
    "        if print_results:\n",
    "            print(f'\\t{\"Message\":{max_len}} {\"Expected\":10} {\"Predicted\":10}')\n",
    "            print((\"\\t\"*1 + \"-\" * max_len) + \" \" + \"-\" * 10 + \" \" + \"-\" * 10)\n",
    "\n",
    "        for label, emails in test_data.items():\n",
    "            for email in emails:\n",
    "                predicted_label = self.classifier.predict(email)\n",
    "\n",
    "                if print_results:\n",
    "                    print(f'\\t{email:{max_len}} {label:10} {predicted_label:10} {\"WRONG!\" if predicted_label != label else \"CORRECT!\"}')\n",
    "\n",
    "                if predicted_label == label:\n",
    "                    num_correct += 1\n",
    "                num_total += 1\n",
    "\n",
    "        accuracy = num_correct / num_total\n",
    "        \n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the NaiveBayesClassifier.\n",
    "        \"\"\"\n",
    "        self._spam_list = [] # crude: list of spam sentences\n",
    "        self._ham_list = [] # crude: list of ham sentences\n",
    "\n",
    "        self._vocabulary = set() \n",
    "\n",
    "        self._alpha = 1  # Laplace smoothing parameter\n",
    "\n",
    "        self._spam_word_counts = {}\n",
    "        self._ham_word_counts = {}\n",
    "        self._num_spam_words = 0\n",
    "        self._num_ham_words = 0\n",
    "\n",
    "        self._prior_spam = 0\n",
    "        self._prior_ham = 0\n",
    "\n",
    "        self._spam_likelihoods = {}\n",
    "        self._ham_likelihoods = {}\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        This just helps pretty print the class to help understand the internals a little better.\n",
    "        Great for debugging.\n",
    "\n",
    "        :return: a string representation of the classifier.\n",
    "        \"\"\"\n",
    "\n",
    "        # print a table of spam_probs and ham_probs\n",
    "        header = (\"\\t|Words\\t\\t|P(w\\|S)\\t|P(w\\|H)|\\n\")\n",
    "        header +=  (\"\\t|--------------|---------------|---------------|\\n\") \n",
    "\n",
    "        for word in vocabulary:\n",
    "            header += (f\"\\t|{word:10s}\\t|{spam_likelihoods[word]:.4f}|\\t\\t{ham_likelihoods[word]:.4f}|\\n\")\n",
    "\n",
    "        return (f\"NB Classifier with {len(self._vocabulary)} words.\\n\\n\"\n",
    "                f\"\\tA priori probabilities: spam={self._prior_spam:.4f}, ham={self._prior_ham:.4f}.\\n\"\n",
    "                f\"\\tAlpha parameter: {self._alpha}\\n\"\n",
    "                f\"{header}\")\n",
    "\n",
    "    def fit(self, spam_list: list, ham_list: list):\n",
    "        \"\"\"\n",
    "        Train the classifier with the provided spam and ham lists.\n",
    "        \n",
    "        :param spam_list: List of spam sentences.\n",
    "        :param ham_list: List of ham sentences.\n",
    "        \"\"\"\n",
    "        self._spam_list = spam_list\n",
    "        self._ham_list = ham_list\n",
    "\n",
    "        # set all words in both list to lower (and do any other filtering\n",
    "        for i in range(len(self._spam_list)):\n",
    "            self._spam_list[i] = self._spam_list[i].lower()\n",
    "\n",
    "        for i in range(len(self._ham_list)):\n",
    "            self._ham_list[i] = self._ham_list[i].lower()\n",
    "        \n",
    "        self._vocabulary = self._build_vocabulary()\n",
    "        \n",
    "        self._spam_word_counts, self._ham_word_counts = self._build_word_counts()\n",
    "        \n",
    "        self._num_spam_words = sum(self._spam_word_counts.values())\n",
    "        self._num_ham_words  = sum(self._ham_word_counts.values())\n",
    "        \n",
    "        # Complute the priors\n",
    "        self._prior_spam = self._num_spam_words / (self._num_spam_words + self._num_ham_words)\n",
    "        self._prior_ham  = 1 - self._prior_spam\n",
    "        \n",
    "        # Compute the likelihoods\n",
    "        self._spam_likelihoods = self._compute_likelihoods(self._spam_word_counts, self._num_spam_words)\n",
    "        self._ham_likelihoods  = self._compute_likelihoods(self._ham_word_counts,  self._num_ham_words)\n",
    "\n",
    "\n",
    "    def update(self, new_spam: list, new_ham: list):\n",
    "        \"\"\"\n",
    "        Update the classifier with new spam and ham messages.\n",
    "\n",
    "        :param new_spam: List of new spam messages.\n",
    "        :param new_ham: List of new ham messages.\n",
    "        \"\"\"\n",
    "\n",
    "        # set all words in both list to lower (and \n",
    "        # do any other filtering). \n",
    "        new_spam[:] = [word.lower() for word in new_spam]\n",
    "        new_ham[:] = [word.lower() for word in new_ham]\n",
    "\n",
    "        # Add the new messages to the existing \n",
    "        # spam and ham corpus.\n",
    "        self._spam_list.extend(new_spam)\n",
    "        self._ham_list.extend(new_ham)\n",
    "\n",
    "        # Now refit the priors and likelihoods\n",
    "        self.fit(self._spam_list, self._ham_list)\n",
    "\n",
    "    def predict(self, msg: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify a single email as spam or ham.\n",
    "\n",
    "        :param msg: The email message to classify.\n",
    "        :return: Classification as 'SPAM' or 'HAM'.\n",
    "        \"\"\"\n",
    "        words = msg.split()\n",
    "\n",
    "        #set all words in words to lower\n",
    "        words[:] = [word.lower() for word in words]\n",
    "\n",
    "        # initiallise with the priors\n",
    "        spam_posterior_prob = self._prior_spam\n",
    "        ham_posterior_prob = self._prior_ham\n",
    "\n",
    "        # then multiply the likelhoods\n",
    "        for word in words:\n",
    "            spam_posterior_prob *= self._spam_likelihoods.get(word, self._alpha / self._num_spam_words)       \n",
    "            ham_posterior_prob *= self._ham_likelihoods.get(word, self._alpha / self._num_ham_words)\n",
    "\n",
    "        return \"spam\" if spam_posterior_prob > ham_posterior_prob else \"ham\" \n",
    " \n",
    "    def _build_vocabulary(self) -> set:\n",
    "        \"\"\"\n",
    "        Build a vocabulary from the spam and ham lists.\n",
    "\n",
    "        :return: The vocabulary as a set (unique words).\n",
    "        \"\"\"\n",
    "        vocabulary = set()\n",
    "        for email in self._spam_list + self._ham_list:\n",
    "            words = email.split()\n",
    "            for word in words:\n",
    "                vocabulary.add(word.lower())\n",
    "                \n",
    "        return vocabulary\n",
    "\n",
    "    def _build_word_counts(self) -> (dict, dict):\n",
    "        \"\"\"\n",
    "        Build dictionaries of word counts for spam and ham.\n",
    "\n",
    "        :return: Dictionaries of word counts.\n",
    "        \"\"\"\n",
    "        spam_word_counts = {word: 0 for word in self._vocabulary}\n",
    "        ham_word_counts = {word: 0 for word in self._vocabulary}\n",
    "\n",
    "        for email in self._spam_list:\n",
    "            for word in email.split():\n",
    "                if word in spam_word_counts:\n",
    "                    spam_word_counts[word] += 1\n",
    "\n",
    "        for email in self._ham_list:\n",
    "            for word in email.split():\n",
    "                if word in ham_word_counts:\n",
    "                    ham_word_counts[word] += 1\n",
    "\n",
    "        return spam_word_counts, ham_word_counts\n",
    "\n",
    "    def _compute_likelihoods(self, word_counts: dict, total_words: int) -> dict:\n",
    "        \"\"\"\n",
    "        Compute the likelihood of each word. Returns a new \n",
    "        dictionary that gives smoothed relative frequencies \n",
    "        for each word in the word_counts dictionary. \n",
    "        The smoothing is done using the self._alpha parameter.\n",
    "\n",
    "        :param word_counts: The frequency of each word.\n",
    "        :param total_words: Total number of words.\n",
    "        :return: Dictionary of likelihoods.\n",
    "        \"\"\"\n",
    "        likelihoods = {}\n",
    "        for word, count in word_counts.items():\n",
    "            smoothed_count = count + self._alpha\n",
    "            likelihood = smoothed_count / total_words\n",
    "            likelihoods[word] = likelihood\n",
    "\n",
    "        return likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier with 15 words.\n",
      "\n",
      "\tA priori probabilities: spam=0.6087, ham=0.3913.\n",
      "\tAlpha parameter: 1\n",
      "\t|Words\t\t|P(w\\|S)\t|P(w\\|H)|\n",
      "\t|--------------|---------------|---------------|\n",
      "\t|us        \t|0.2143|\t\t0.1111|\n",
      "\t|benefits  \t|0.0714|\t\t0.2222|\n",
      "\t|send      \t|0.2857|\t\t0.1111|\n",
      "\t|our       \t|0.1429|\t\t0.1111|\n",
      "\t|website   \t|0.1429|\t\t0.1111|\n",
      "\t|the       \t|0.0714|\t\t0.2222|\n",
      "\t|your      \t|0.2857|\t\t0.2222|\n",
      "\t|report    \t|0.0714|\t\t0.2222|\n",
      "\t|physical  \t|0.0714|\t\t0.2222|\n",
      "\t|importance\t|0.0714|\t\t0.2222|\n",
      "\t|account   \t|0.1429|\t\t0.1111|\n",
      "\t|review    \t|0.1429|\t\t0.1111|\n",
      "\t|password  \t|0.2143|\t\t0.1111|\n",
      "\t|vows      \t|0.0714|\t\t0.2222|\n",
      "\t|activity  \t|0.0714|\t\t0.3333|\n",
      "\n",
      "\n",
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\trenew your password                 spam       spam       CORRECT!\n",
      "\trenew your vows                     spam       ham        WRONG!\n",
      "\tbenefits of our account             ham        ham        CORRECT!\n",
      "\tthe importance of physical activity ham        ham        CORRECT!\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "print(f\"{nb_classifier}\\n\")\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "#How good is the classifier?\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f\"\\nAccuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the fitting function is not a once-off operation. You can constantly update the priors and likehoods as you get new information (i.e. corrected labelled data). This is almost how human's work.\n",
    "\n",
    "Here I demonstrate a super-naive way to update an existing model and re-test. Again, there's no notion of serialisation/deserialisation so if the classifier goes out of scope, you have to rebuild everything from scratch. There really are better ways to do this... this is just for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for:\n",
      "\n",
      "\t\"Give me your password\": spam\n",
      "\t\"Send me your data\": spam\n",
      "\t\"Give me your money\": spam\n",
      "\t\"I would love a car\": ham\n"
     ]
    }
   ],
   "source": [
    "# Update the classifier with new labelled data (i'm just using the test set as it's convenient)\n",
    "nb_classifier.update(new_emails['spam'], new_emails['ham'])\n",
    "\n",
    "# Present new, unknown, message data to the classifier and \"see what happens\"\n",
    "mew_msgs = [\n",
    "    \"Give me your password\",\n",
    "    \"Send me your data\",\n",
    "    \"Give me your money\",\n",
    "    \"I would love a car\"\n",
    "]\n",
    "\n",
    "print(\"Predictions for:\\n\")\n",
    "for msg in mew_msgs:\n",
    "    result = nb_classifier.predict(msg)\n",
    "    print(f\"\\t\\\"{msg}\\\": {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's debatable if these labels are correct, but it is what the system predicts, once updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment demonstrates the use of bayes theorem in the binary classification of messages. This is only a toy implementation. You would not use this in the real-world.\n",
    "\n",
    "**Aside:** it should be noted that raw text is only one part of spam detection. Modern systems are now much more sophisticated and take into account, not just the text, but the message format, hyperlinks, images, point of origin (email spoofing) and more.\n",
    "\n",
    "### Sources\n",
    "I used all of the references listed below to build this classifier.\n",
    "\n",
    "### Feedback\n",
    "Deventhiran Ranganathan suggested I pre-process the code to ensure consistent lower case. This is a good idea. I had made reference to it in the section on stopwords, punctuation, spelling issues and case. \n",
    "\n",
    "Generally, it's a really good idea, for any software to make sure you constrain the input to avoid accidental or maliciously motivated crashes (security by design). I added the to_lower function to all data ingress points; it had no effect on the outcome (only re-balanced the internal likelihoods).\n",
    "\n",
    "### System Design\n",
    "The NaiveBayesClassifier class allows:\n",
    "\n",
    "* Training (\"fitting\") an NB classifier. Training (or fitting) generates the priors and likelihoods for the data set.\n",
    "* Updates the priors and likelihoods based on new, labelled data.\n",
    "* Predicting the class (SPAM or HAM) of a new message.\n",
    "\n",
    "The ClassifierTester class enables us to establish the accuracy of the classifier using labelled, but previously, unseen data.\n",
    "\n",
    "The code is cluttered with print statements to show intermediate workings, for debugging and to aid intuition. In the real world, I would remove this and create unit tests to confirm this behaviour. \n",
    "\n",
    "#### Advantages \n",
    "* Surprisingly good even if it assumes all words are independant (which is mostly, clearly not the case in language).\n",
    "* Easy to understand.\n",
    "* Relatively quick to implement.\n",
    "\n",
    "#### Disadvantages\n",
    "* The NB classifier does not incorporate the order of words (or context).\n",
    "* We assume all input is lowercase. There's no facility for stopwords.\n",
    "* The system has no notion of punctuation or spelling mistakes.\n",
    "* the previous two bullets mean we are ignoring critical information.\n",
    "* The current design calls for the NB class to retain the labelled data that it has previously seen. This is poor as, once the object goes out of scope, the GC will clean it up and all training data will be lost. Ideally, we would at the very least, save the core data to disk and deserialise when we reinstantiate an opbject.\n",
    "\n",
    "### Discussion\n",
    "#### Not using a Threshold\n",
    "I classify based on comparing the relative posterior probabilities (picking the highest as the most likely). \n",
    "I prefer this as using thresholds always raises the question of what threshold should I set; i.e. how good is good enough.\n",
    "\n",
    "#### Text Pre-processing: Remove stopwords\n",
    "Removing stop words actually dropped the accuracy down to 50/50. In this case, with so few words, removing stops words may actually remove information from the train and test sets. More specialised toolkits, like ntlk, have modules with support for removing stops words (for english and many other languages).\n",
    "\n",
    "#### Text Pre-processing: Remove punctuation, force lower case, stemming and so on\n",
    "We could pre-process the test to remove punctuation and force all text to be lowercase. \n",
    "\n",
    "#### Accuracy (with different test/train splits or with new data)\n",
    "I have not gone into great detail with metrics; only providing the accuracy. Note that the accuracy of the system can be affected by changing how we divide the training and tests set. See here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\tsend us your password               spam       spam       CORRECT!\n",
      "\trenew your password                 spam       spam       CORRECT!\n",
      "\tbenefits physical activity          ham        ham        CORRECT!\n",
      "\tthe importance of physical activity ham        ham        CORRECT!\n",
      "\n",
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# take a SPAM message from previous and move to test and vice versa.\n",
    "previous_spam = ['renew your vows', 'review our website', 'send your password', 'send us your account']\n",
    "previous_ham = ['benefits of our account','Your activity report', 'the importance vows']\n",
    "new_emails = {'spam':['send us your password', 'renew your password' ], 'ham':['benefits physical activity', 'the importance of physical activity']}\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i get an accuracy of 1.0??? This is very suspect and highlights a real problem with the methodology! The same labelled messages exist in both test and train sets, I've just changed what I used to test and train with; and I get a different accuracy depending on the split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing the test set size by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\trenew your password                 spam       spam       CORRECT!\n",
      "\trenew your vows                     spam       ham        WRONG!\n",
      "\tbenefits of our account             ham        ham        CORRECT!\n",
      "\tyour exam results are in            ham        ham        CORRECT!\n",
      "\tthe importance of physical activity ham        ham        CORRECT!\n",
      "\n",
      "Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "previous_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
    "previous_ham = ['Your activity report','benefits physical activity', 'the importance vows']\n",
    "new_emails = {'spam':['renew your password', 'renew your vows'], 'ham':['benefits of our account', 'your exam results are in', 'the importance of physical activity']}\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bags of Words and (Not) Understanding Context\n",
    "Note that this approach is naive in many ways. It assumes the features are independance. Consider what happens if we jumble up the words in our training and test data. This is shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\tyour renew  password                spam       spam       CORRECT!\n",
      "\trenew vows your                     spam       ham        WRONG!\n",
      "\tour benefits of account             ham        ham        CORRECT!\n",
      "\tthe importance of physical activity ham        ham        CORRECT!\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "previous_spam = ['us send password your', 'website our review', 'your password send', 'send us your account']\n",
    "previous_ham = ['activity report Your', 'benefits activity physical', 'the vows importance']\n",
    "new_emails = {'spam': ['your renew  password', 'renew vows your'], 'ham': ['our benefits of account', 'the importance of physical activity']}\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result. This is not an exhaustive test but demonstrates the intuition that the method ignores word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Naive Bayes, Clearly Explain!!!](https://www.youtube.com/watch?v=O2L2Uv9pdDA&ab_channel=StatQuestwithJoshStarmer)\n",
    "* [Bayes theorem, the geometry of changing beliefs](https://www.youtube.com/watch?v=HZGCoVF3YvM&ab_channel=3Blue1Brown)\n",
    "* [Notes on Naive Bayes Classifiers for Spam Filtering](https://courses.cs.washington.edu/courses/cse312/18sp/lectures/naive-bayes/naivebayesnotes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
