{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook E-tivity 3 CE4021 Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student name:** Jason Coleman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID:** 9539719"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you believe required imports are missing, please contact your moderator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below information to create a Naive Bayes SPAM filter. Test your filter using the messages in new_emails. You may add as many cells as you require to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
    "previous_ham = ['Your activity report','benefits physical activity', 'the importance vows']\n",
    "new_emails = {'spam':['renew your password', 'renew your vows'], 'ham':['benefits of our account', 'the importance of physical activity']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This will be a very naive Naive Bayes classifier. It will follow a so-called `Bag-of-words` approach where I will classify a message as `SPAM` or `HAM` based on the presence of the words in the string. Notably, the bag of words does not consider the context in which the work is used (i.e. you can reorder the words into complete nonsense and get the same result). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Bayes Theorem\n",
    "Let us start with the formula for Bayes Theorem. \n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "* where $P(A \\mid B)$ is the probability of event A happening given that event B has happened.\n",
    "* and $P(B \\mid A)$ is the probability of event B given A.\n",
    "* and $P(A)$ and $P(B)$ are the probabilities of events A and B, respectively.\n",
    "\n",
    "In the context of our spam filter:\n",
    "\n",
    "\n",
    "*  where $A$ could be the event \"The email is spam\".\n",
    "*  and $B$ could be \"The email contains the word 'pAssword'\".\n",
    "\n",
    "### Naive Bayes Classifier (NB)\n",
    "\n",
    "NB is a classification technique based on Bayes' theorem; with the \"naive\" assumption that every pair of features is independent of each other. Even though it is caled naive, it can perform surpisingly well. I worked as an analysts in the AV industry a few years ago and, in the early days, when we had less data (and features) NB worked really well. \n",
    "\n",
    "Reframing this a little, in the context of SPAM/HAM.\n",
    "\n",
    "* Where $S$ represents the event that an email is spam.\n",
    "* and $W$ represents the event that a specific word appears in the email.\n",
    "\n",
    "We can write:\n",
    "\n",
    "$$\n",
    "P(S \\mid W) = \\frac{P(W \\mid S) \\times P(S)}{P(W)}\n",
    "$$\n",
    "\n",
    "\n",
    "*  Where $P(S \\mid W)$ is the probability that an email is spam given that it contains the word $W$.\n",
    "*  and $P(W \\mid S)$ is the probability that the word $W$ appears in a spam email.\n",
    "*  and $P(S)$ is the overall probability (or prior) that any email is spam. This is a `prior`. \n",
    "*  and $P(W)$ is the probability that the word $W$ appears in any email, regardless of it being spam or not. This is a `prior`. \n",
    "\n",
    "#### Working on messages with more than one word\n",
    "\n",
    "Emails are rarely composed of just one word so our implemnentation will need to account for all of the words in the message. So, for a given email message, $m$, consisting of words $w_1$, $w_2$, $\\ldots$, $w_n$, we need to modify the method to account for multiple words. \n",
    "\n",
    "0. We will need to calculate the likelihoods for words being both SPAM or HAM. That is: $P(w_1 | S)$, $P(w_1 | H)$ for each word int he training data. \n",
    "\n",
    "Then, for the training messages:\n",
    "\n",
    "1. Calculate the likelihood of the email being Spam, based on word frequncy:\n",
    "\n",
    "$$\n",
    "P(m | S) = P(w_1 | S) \\times P(w_2 | S) \\times \\ldots \\times P(w_n | S)\n",
    "$$\n",
    "\n",
    "2. Calculate the likelihood of the email being Ham, based on word frequncy:\n",
    "\n",
    "$$\n",
    "P(m | H) = P(w_1 | H) \\times P(w_2 | H) \\times \\ldots \\times P(w_n | H)\n",
    "$$\n",
    "\n",
    "Then, finally, calculate the posterior probabilitites for being Ham and Spam.\n",
    "\n",
    "$$\n",
    "P(S | m) = \\frac{P(m | S) \\times P(S)}{P(m)}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "P(H | m) = \\frac{P(m | H) \\times P(H)}{P(m)}\n",
    "$$\n",
    "\n",
    "* Where $P(S)$ and $P(H)$ are the prior probabilities of a message being spam and ham, respectively (we know this from the initial training set).\n",
    "* and $P(m)$ is the total probability of observing message $m$. \n",
    "\n",
    "Note, I don't need to calculate $P(m)$ if I am comparing the relative probability of $P(S | m)$ and $P(H | m)$ so the method I will implement will actually be the following:\n",
    "\n",
    "$$\n",
    "P(S | m) = P(w_1 | S) \\times P(w_2 | S) \\times \\ldots \\times P(w_n | S) \\times P(S)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "P(H | m) = P(w_1 | H) \\times P(w_2 | H) \\times \\ldots \\times P(w_n | H)\\times P(H)\n",
    "$$\n",
    "\n",
    "#### Classification\n",
    "Then, given a new message, if $P(S | m) > P(H | m)$, I will classify the message as SPAM, else I will classify the message as Ham. \n",
    "\n",
    "By doing this, I am side-stepping using a threshold and just comparing the relative posterior probabilitites for the message being HAM and SPAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "We will need to preprocess the data a little; to get it into the right shape. We will start by getting the components of the Bayes formula: the `priors` and the `likelihoods` for the words within the messages. \n",
    "\n",
    "**Tokenise:** break each message into words. Each word is called a `token`. It's the fundemental unit of classification in a Bag of Words classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to calculate Priors and likelihoods. To do this we first need to construct a vocabulary. \n",
    "\n",
    "**Notes**:\n",
    "\n",
    "* I will reference the steps in the reference implementation\n",
    "* My implementation will use list and dictionary comprehensions to make the code more concise.\n",
    "* I will print intermediate output to aid understanding and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary\n",
    "The prior probability of an email being spam or ham is calculated based on the proportion of spam or ham emails in the training data.\n",
    "\n",
    "1. Create a set called *vocabulary*.\n",
    "2. For each email in the SPAM and HAM training messages, split the words\n",
    "3. Add all words to the set.\n",
    "\n",
    "We now have a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = set()\n",
    "\n",
    "for email in previous_spam + previous_ham:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        vocabulary.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probability of an email being spam or ham is calculated based on the proportion of spam or ham emails that exists in the training data.\n",
    "\n",
    "$$\n",
    "\\texttt{prior\\_spam} = \\frac{\\texttt{num\\_spam}}{\\texttt{total}}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\texttt{prior\\_ham} = \\frac{\\texttt{num\\_ham}}{\\texttt{total}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Spam: 0.5714285714285714\n",
      "Prior Ham: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "num_spam = len(previous_spam)\n",
    "num_ham = len(previous_ham)\n",
    "\n",
    "total = num_spam + num_ham\n",
    "\n",
    "prior_spam = num_spam / total\n",
    "prior_ham = num_ham / total\n",
    "\n",
    "print(f\"Prior Spam: {prior_spam}\")\n",
    "print(f\"Prior Ham: {prior_ham}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the Conditional Probabilitites (aka Likelihoods)\n",
    "Here, for each word in the vocabulary, the probability of the word given that an email is spam or ham is calculated. We do this by:\n",
    "\n",
    "1. Counting the occurrences of each word in both the spam and ham emails.\n",
    "2. Apply Laplace Smoothing to avoid zero probabilities. If we don't add this, we will eventually end up multiplying by zero (as the word has never been seen before, the conditional probability willk be zero).\n",
    "3. Calculate the conditional probabilities (likelihoods) for each word being both SAPM and HAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: \n",
      "{'send', 'Your', 'physical', 'report', 'benefits', 'importance', 'us', 'our', 'activity', 'account', 'website', 'the', 'vows', 'password', 'your', 'review'}\n",
      "\n",
      "Spam/HAM dictionaries are as follows:\n",
      "\n",
      "\tWord\t\tSpam Count \tHam Count\n",
      "\t----------------------------------------\n",
      "\tsend      \t 3\t\t 0\n",
      "\tYour      \t 0\t\t 1\n",
      "\tphysical  \t 0\t\t 1\n",
      "\treport    \t 0\t\t 1\n",
      "\tbenefits  \t 0\t\t 1\n",
      "\timportance\t 0\t\t 1\n",
      "\tus        \t 2\t\t 0\n",
      "\tour       \t 1\t\t 0\n",
      "\tactivity  \t 0\t\t 2\n",
      "\taccount   \t 1\t\t 0\n",
      "\twebsite   \t 1\t\t 0\n",
      "\tthe       \t 0\t\t 1\n",
      "\tvows      \t 0\t\t 1\n",
      "\tpassword  \t 2\t\t 0\n",
      "\tyour      \t 3\t\t 0\n",
      "\treview    \t 1\t\t 0\n",
      "\n",
      "The condtional probabilities (likelihoods) for spam and ham are as follows:\n",
      "\n",
      "\tWord\t\tP(word|spam)\tP(word|ham)\n",
      "\t----------------------------------------\n",
      "\tsend      \t0.0167\t\t0.0063\n",
      "\tYour      \t0.0042\t\t0.0125\n",
      "\tphysical  \t0.0042\t\t0.0125\n",
      "\treport    \t0.0042\t\t0.0125\n",
      "\tbenefits  \t0.0042\t\t0.0125\n",
      "\timportance\t0.0042\t\t0.0125\n",
      "\tus        \t0.0125\t\t0.0063\n",
      "\tour       \t0.0083\t\t0.0063\n",
      "\tactivity  \t0.0042\t\t0.0187\n",
      "\taccount   \t0.0083\t\t0.0063\n",
      "\twebsite   \t0.0083\t\t0.0063\n",
      "\tthe       \t0.0042\t\t0.0125\n",
      "\tvows      \t0.0042\t\t0.0125\n",
      "\tpassword  \t0.0125\t\t0.0063\n",
      "\tyour      \t0.0167\t\t0.0063\n",
      "\treview    \t0.0083\t\t0.0063\n"
     ]
    }
   ],
   "source": [
    "# Create dictionaries from the vocabulary to store word counts, initialise the count to zero\n",
    "spam_word_counts = {word: 0 for word in vocabulary}\n",
    "ham_word_counts = {word: 0 for word in vocabulary}\n",
    "print(f\"Vocabulary: \\n{vocabulary}\\n\")\n",
    "\n",
    "# Determine frequencies of words in spam and ham emails (I cosider repeatred words here)\n",
    "for email in previous_spam:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        spam_word_counts[word] += 1\n",
    "\n",
    "for email in previous_ham:\n",
    "    words = email.split()\n",
    "    for word in words:\n",
    "        ham_word_counts[word] += 1\n",
    "\n",
    "print(\"Spam/HAM dictionaries are as follows:\\n\")\n",
    "\n",
    "# print the dictionaries for spam ham in one table\n",
    "print('\\tWord\\t\\tSpam Count \\tHam Count')\n",
    "print('\\t'+'-' * 40)\n",
    "for word in vocabulary:\n",
    "    print(f'\\t{word:10s}\\t{spam_word_counts[word]:2d}\\t\\t{ham_word_counts[word]:2d}')\n",
    "\n",
    "print()\n",
    "\n",
    "#  Preparing the (smoothed) likelihoods\n",
    "alpha = 1 # Laplace smoothing (in this case 'add-one' smoothing) parameter\n",
    "\n",
    "spam_total_words = (sum(spam_word_counts.values()) + alpha) * len(vocabulary)\n",
    "ham_total_words = (sum(ham_word_counts.values()) + alpha) * len(vocabulary)\n",
    "\n",
    "# Compute the likeloods for SPAM/HAM.\n",
    "spam_likelihoods = {word: (count + alpha) / spam_total_words for word, count in spam_word_counts.items()}\n",
    "ham_likelihoods = {word: (count + alpha) / ham_total_words for word, count in ham_word_counts.items()}\n",
    "\n",
    "print(\"The condtional probabilities (likelihoods) for spam and ham are as follows:\\n\")\n",
    "\n",
    "# print a table of spam_probs and ham_probs\n",
    "print('\\tWord\\t\\tP(word|spam)\\tP(word|ham)')\n",
    "print('\\t'+'-' * 40)\n",
    "for word in vocabulary:\n",
    "    print(f'\\t{word:10s}\\t{spam_likelihoods[word]:.4f}\\t\\t{ham_likelihoods[word]:.4f}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Classifier\n",
    "We can now test the classfier on the test set of data. We use the previously calculated priors and likelihood to calculate the posterior probabilities for both SPAM and HAM and this will allow me to classify new emails as spam or ham. The classification is done by comparing the posterior probability for Ham to that of SPAM: the highest one is the output label.\n",
    "\n",
    "We can assess the accuracy of the classifier against data it has not seen before. We do this with a labelled test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "renew your password            spam\n",
      "renew your vows                ham\n",
      "benefits of our account        ham\n",
      "the importance of physical activity ham\n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "num_total = 0\n",
    "\n",
    "# for all labelled messages in the test set, grouped by label\n",
    "for label, emails in new_emails.items():\n",
    "    for email in emails:\n",
    "        words = email.split()\n",
    "\n",
    "        # initialise with the priors\n",
    "        spam_posterior_prob = prior_spam\n",
    "        ham_posterior_prob = prior_ham\n",
    "        \n",
    "        # Caculate the posterior probabilities based on the multiplied likelihoods\n",
    "        for word in words:\n",
    "            if word in spam_likelihoods:\n",
    "                spam_posterior_prob *= spam_likelihoods[word]\n",
    "            else:\n",
    "                spam_posterior_prob *= alpha / spam_total_words\n",
    "            if word in ham_likelihoods:\n",
    "                ham_posterior_prob *= ham_likelihoods[word]\n",
    "            else:\n",
    "                ham_posterior_prob *= alpha / ham_total_words\n",
    "                \n",
    "        # I think it is better to compare the relative probabilities instead of\n",
    "        # picking a threshold. You always get into a trade-off between false\n",
    "        # positives and false negatives.        \n",
    "        if spam_posterior_prob > ham_posterior_prob:\n",
    "            predicted_label = 'spam'\n",
    "        else:\n",
    "            predicted_label = 'ham'\n",
    "\n",
    "        print(f'{email:30s} {predicted_label}')\n",
    "\n",
    "        if predicted_label == label:\n",
    "            num_correct += 1\n",
    "\n",
    "        num_total += 1\n",
    "\n",
    "accuracy = num_correct / num_total\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this work?\n",
    "To understand how this works, let's take an example message, `renew your password` from the SPAM test set. Let's start with the word `renew`.\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\text{spam}) = \\text{prior\\_spam}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\text{ham}) = \\text{prior\\_ham}\n",
    "$$\n",
    "\n",
    "And\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\text{renew} | \\text{spam}) = \\text{spam\\_likelihoods[renew]}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "P(\\text{renew} \\mid \\text{ham}) = \\text{ham\\_likelihoods[renew]}\n",
    "$$\n",
    "\n",
    "\n",
    "We do this for every word int he message, multiplying the spam and ham likelihoods for the words int he message. If the word is not present in our training data, we still give it a non-zero probability because of the Laplace smoothing. This ensures our model can handle unseen words (if we dont do this, we will end up multiplying a zero into the spam_posterior_prob and/or ham_posterior_prob term).\n",
    "\n",
    "Finally, after computing the probabilities for each word, if the summed probability of the email being spam is higher than that of it being ham, the email is classified as spam, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's package all of the previous code into classes, so it's more modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    \"\"\"\n",
    "    Create a NB classifier for 2 classes: spam and ham.\n",
    "\n",
    "    Note that I have tried to break the methods down to reduce what\n",
    "    my linter calls Conginitive Complexity. This makes it easier to\n",
    "    understand and test the code.\n",
    "\n",
    "    I have two prediction methods:\n",
    "    - predict: returns the label of the email based on relative probabilities\n",
    "    - predict_t: returns the label of the email based on a threshold\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.__vocabulary = set()\n",
    "        self.__spam_word_counts = {}\n",
    "        self.__ham_word_counts = {}\n",
    "        self.__spam_likelihoods = {}\n",
    "        self.__ham_likelihoods = {}\n",
    "        self.__spam_prior = 0\n",
    "        self.__ham_prior = 0\n",
    "        self.__alpha = 1\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        This just helps me pretty print the class \n",
    "        to help understand the internals a little better\n",
    "        \n",
    "        :return: a string representation of the classifier\n",
    "        \"\"\"\n",
    "        return (f'NB Classifier with {len(self.__vocabulary)} words.\\n'\n",
    "                f'\\tA priori probabilities: spam={self.__spam_prior}, ham={self.__ham_prior}.\\n'\n",
    "                f'\\tAlpha parameter: {self.__alpha}')\n",
    "    \n",
    "\n",
    "    def fit(self, spam_emails: list, ham_emails: list) -> None:\n",
    "        \"\"\"\n",
    "        Train the classifier with spam and ham emails. \n",
    "\n",
    "        Throw an exception if there are no words in the vocabulary\n",
    "\n",
    "        :param spam_emails: a list of spam emails\n",
    "        :param ham_emails: a list of ham emails\n",
    "        :param remove_stop_words: remove stopwords from the emails\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1\n",
    "        self.__build_priors(spam_emails, ham_emails)\n",
    "        # Step 2\n",
    "        self.__build_laplacian_likelihoods(spam_emails, ham_emails)\n",
    "        \n",
    "    \n",
    "    def __build_priors(self, spam_emails: list, ham_emails: list) -> None:\n",
    "        \"\"\"\n",
    "        Compute the prior probabilities for spam and ham emails.\n",
    "\n",
    "        :param spam_emails: a list of spam emails\n",
    "        :param ham_emails: a list of ham emails\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        num_spam = len(spam_emails)\n",
    "        num_ham = len(ham_emails)\n",
    "\n",
    "        total = num_spam + num_ham\n",
    "\n",
    "        self.__spam_prior = num_spam / total\n",
    "        self.__ham_prior = num_ham / total\n",
    "        \n",
    "\n",
    "    def __build_word_frequency(self, emails: list, word_counts: dict) -> None:\n",
    "        \"\"\"\n",
    "        Update word counts for the provided emails.\n",
    "        Update the vocabulary as well.\n",
    "        \n",
    "        :param emails: a list of emails\n",
    "        :param word_counts: a dictionary to store word counts\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for email in emails:\n",
    "            words = email.split()\n",
    "            for word in words:\n",
    "                self.__vocabulary.add(word)\n",
    "                if word in word_counts:\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "\n",
    "\n",
    "    def __compute_smoothed_likelihoods(self, spam_total_words: int, ham_total_words: int) -> None:\n",
    "        \"\"\"\n",
    "        Compute smoothed likelihoods for SPAM and HAM, for each word in the vocabulary.\n",
    "\n",
    "        :param spam_total_words: total number of words in spam emails\n",
    "        :param ham_total_words: total number of words in ham emails\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        for word in self.__vocabulary:\n",
    "            self.__spam_likelihoods[word] = (self.__spam_word_counts.get(word, 0) + self.__alpha) / spam_total_words\n",
    "            self.__ham_likelihoods[word] = (self.__ham_word_counts.get(word, 0) + self.__alpha) / ham_total_words \n",
    "\n",
    "\n",
    "    def __build_laplacian_likelihoods(self, spam_emails: list, ham_emails: list) -> None:\n",
    "        \"\"\"\n",
    "        Calculate the likelihoods for spam and ham emails using Laplacian smoothing.\n",
    "\n",
    "        :param spam_emails: a list of spam emails\n",
    "        :param ham_emails: a list of ham emails\n",
    "        :return: None \n",
    "        \"\"\"\n",
    "        self.__build_word_frequency(spam_emails, self.__spam_word_counts)\n",
    "        self.__build_word_frequency(ham_emails, self.__ham_word_counts)\n",
    "\n",
    "        self.__alpha = 1\n",
    "        vocabulary_len = len(self.__vocabulary)\n",
    "\n",
    "        spam_total_words = sum(self.__spam_word_counts.values()) + self.__alpha * vocabulary_len\n",
    "        ham_total_words = sum(self.__ham_word_counts.values()) + self.__alpha * vocabulary_len\n",
    "\n",
    "        if spam_total_words == 0 or ham_total_words == 0:\n",
    "            raise ValueError('No words in the vocabulary')\n",
    "\n",
    "        self.__compute_smoothed_likelihoods(spam_total_words, ham_total_words)\n",
    "\n",
    "    def __compute_probability(self, words: list, likelihoods: dict, prior: float) -> float:\n",
    "        \"\"\"\n",
    "        Compute the probability of a list of words given the likelihoods and prior.\n",
    "        \n",
    "        :param words: a list of words\n",
    "        :param likelihoods: a dictionary of likelihoods\n",
    "        :param prior: the prior probability\n",
    "        :return: the probability\n",
    "        \"\"\"\n",
    "        prob = prior\n",
    "        total_words = sum(likelihoods.values()) + self.__alpha * len(self.__vocabulary)\n",
    "        for word in words:\n",
    "            if word in likelihoods:\n",
    "                prob *= likelihoods[word]\n",
    "            else:\n",
    "                prob *= self.__alpha / total_words\n",
    "        return prob\n",
    "\n",
    "\n",
    "    def predict(self, email:str) -> str:\n",
    "        \"\"\"\n",
    "        Classify an email as spam or ham\n",
    "\n",
    "        :param email: the email to classify\n",
    "        :return: the label of the email\n",
    "        \"\"\"\n",
    "\n",
    "        words = email.split()\n",
    "        spam_prob = self.__compute_probability(words, self.__spam_likelihoods, self.__spam_prior)\n",
    "        ham_prob = self.__compute_probability(words, self.__ham_likelihoods, self.__ham_prior)\n",
    "\n",
    "        return 'spam' if spam_prob > ham_prob else 'ham'\n",
    "        \n",
    "    def predict_t(self, email:str, threshold: float=0.66) -> str:\n",
    "        \"\"\"\n",
    "        Classify an email as spam or ham\n",
    "\n",
    "        :param email: the email to classify\n",
    "        :param threshold: the threshold to use to classify the email\n",
    "        :return: the label of the email\n",
    "        \"\"\"\n",
    "        words = email.split()\n",
    "        spam_prob = self.__compute_probability(words, self.__spam_likelihoods, self.__spam_prior)\n",
    "        \n",
    "        return 'spam' if spam_prob > threshold else 'ham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierTester:\n",
    "    \"\"\" \n",
    "    The prupose of this class is to test the classifier. If I was doing \n",
    "    this properly, I would use a notion of abstract methods so the\n",
    "    tester could test the accuracy of any approach that uses the same \n",
    "    signatures for fit and predict.\n",
    "    \"\"\"\n",
    "    def __init__(self, classifier):\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def test(self, test_data, print_results=True):\n",
    "        \"\"\"\n",
    "        Test the classifier with test data. This just measures\n",
    "        how many correct predictions the classifier makes (divided\n",
    "        by the total number of possible predictions).     \n",
    "\n",
    "        :param test_data: a dictionary of test data, where \n",
    "                          the key is the label and the value \n",
    "                          is a list of emails\n",
    "        :param print_results: whether to print the results \n",
    "                          (highlight mismatches)\n",
    "        :return: the accuracy of the classifier\n",
    "        \"\"\"\n",
    "        num_correct = 0\n",
    "        num_total = 0\n",
    "\n",
    "        max_len = max([len(email) for emails in test_data.values() for email in emails])\n",
    "\n",
    "        if print_results:\n",
    "            print(f'\\t{\"Message\":{max_len}} {\"Expected\":10} {\"Predicted\":10}')\n",
    "            print(('\\t'*1 + '-' * max_len) + ' ' + '-' * 10 + ' ' + '-' * 10)\n",
    "\n",
    "        for label, emails in test_data.items():\n",
    "            for email in emails:\n",
    "                predicted_label = self.classifier.predict(email)\n",
    "\n",
    "                if print_results:\n",
    "                    print(f'\\t{email:{max_len}} {label:10} {predicted_label:10} {\"WRONG!\" if predicted_label != label else \"\"}')\n",
    "\n",
    "                if predicted_label == label:\n",
    "                    num_correct += 1\n",
    "                num_total += 1\n",
    "\n",
    "        accuracy = num_correct / num_total\n",
    "        \n",
    "        return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Classifier with 16 words.\n",
      "\tA priori probabilities: spam=0.5714285714285714, ham=0.42857142857142855.\n",
      "\tAlpha parameter: 1\n",
      "\n",
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\trenew your password                 spam       spam       \n",
      "\trenew your vows                     spam       spam       \n",
      "\tbenefits of our account             ham        spam       WRONG!\n",
      "\tthe importance of physical activity ham        ham        \n",
      "\n",
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "print(f\"{nb_classifier}\\n\")\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "#How good is the classifier?\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This toy NB bayes approach demonstrates the use or prior knowledge of SPAM/HAM in the classification of new messages. We would constantly update the likehoods as we get new corrected labelled data.\n",
    "\n",
    "!!!!!!!!MORE TODO HERE!!!!!!!\n",
    "\n",
    "\n",
    "### Feedback\n",
    "TODO\n",
    "\n",
    "### System Design (Advantages/Disadvantages)\n",
    "TODO\n",
    "\n",
    "### Using a Threshold\n",
    "I have included two ways to predict whether a message is SPAM or HAM:\n",
    "\n",
    "* Comparingthe relative posterior probabilities (picking the highest as the most likely)\n",
    "* Using a threshold\n",
    "\n",
    "I prefer the former asthe latter approach raises the question of what threshold should I set; i.e. how good is good enough.\n",
    "\n",
    "### Stopwords\n",
    "Removing stop words actually drops the accuracy down to 50/50. \n",
    "\n",
    "### False Positives\n",
    "A few years ago, I worked in the AV industry as a malware analyst and we always had to deal with notions of false positive results (TP, TN, FP, FN); think how many times your AV thought a valid, reputable application was malware. Misclassification is a fact of life with these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bags of Words and Context\n",
    "Note that this approach is naive in many ways. It assumes the features are independance. Consider what happens if we jumble up the words in our training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the classifier...\n",
      "\n",
      "\tMessage                             Expected   Predicted \n",
      "\t----------------------------------- ---------- ----------\n",
      "\tyour renew  password                spam       spam       \n",
      "\trenew vows your                     spam       spam       \n",
      "\tur enhance performance              spam       spam       \n",
      "\tour benefits of account             ham        spam       WRONG!\n",
      "\tYou need to will harder work        ham        spam       WRONG!\n",
      "\tthe importance of physical activity ham        ham        \n",
      "\n",
      "Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "previous_spam = ['us send password your', 'website our review', 'your password send', 'send us your account']\n",
    "previous_ham = ['activity report Your', 'benefits activity physical', 'the vows importance']\n",
    "new_emails = {'spam': ['your renew  password', 'renew vows your', 'ur enhance performance'], 'ham': ['our benefits of account', 'You need to will harder work', 'the importance of physical activity']}\n",
    "\n",
    "# Train the classifier\n",
    "nb_classifier = NaiveBayesClassifier()\n",
    "nb_classifier.fit(previous_spam, previous_ham)\n",
    "\n",
    "# Test the classifier\n",
    "nb_tester = ClassifierTester(nb_classifier)\n",
    "print(\"Testing the classifier...\\n\")\n",
    "\n",
    "accuracy = nb_tester.test(new_emails)\n",
    "print(f'\\nAccuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same result. This method ignores word order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* [Naive Bayes, Clearly Explain!!!](https://www.youtube.com/watch?v=O2L2Uv9pdDA&ab_channel=StatQuestwithJoshStarmer)\n",
    "* [Bayes theorem, the geometry of changing beliefs](https://www.youtube.com/watch?v=HZGCoVF3YvM&ab_channel=3Blue1Brown)\n",
    "* [Notes on Naive Bayes Classifiers for Spam Filtering](https://courses.cs.washington.edu/courses/cse312/18sp/lectures/naive-bayes/naivebayesnotes.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
