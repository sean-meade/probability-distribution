{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook E-tivity 3 CE4021 Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student name: Vaclav Krol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student ID: 23307102"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "#None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you believe required imports are missing, please contact your moderator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below information to create a Naive Bayes SPAM filter. Test your filter using the messages in new_emails. You may add as many cells as you require to complete the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_spam = ['send us your password', 'review our website', 'send your password', 'send us your account']\n",
    "previous_ham = ['Your activity report','benefits physical activity', 'the importance vows']\n",
    "new_emails = {'spam':['renew your password', 'renew your vows'], 'ham':['benefits of our account', 'the importance of physical activity']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Theorem\n",
    "\n",
    "In probability theory and statistics, Bayes' theorem (alternatively Bayes' law or Bayes' rule), describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "Bayes' theorem is stated mathematically as the following equation:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "where A and B are events and ${\\displaystyle P(B)\\neq 0}$\n",
    "\n",
    "* $P(A \\mid B)$ is the conditional probability of event A given that event B is true\n",
    "* $P(B \\mid A)$ is the conditional probability of event B given that event A is true\n",
    "* $P(A)$ and $P(B)$ are the probabilities of observing A and B respectively without any given conditions; they are known as the prior probability and marginal probability.\n",
    "\n",
    "\n",
    "### Naive Bayes classifier\n",
    "\n",
    "It is a classification technique based on Bayes’ Theorem with an independence assumption among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Despite this simplification, the algorithm achieves high accuracy levels.\n",
    "\n",
    "This is the Naive Bayes formula:\n",
    "\n",
    "$$P(S | x1, . . . , xn) ≈ \\frac{P(S) \\prod_{i=1}^n P(x_i | S)}{P(S) \\prod_{i=1}^n P(x_i | S)\\quad + \\quad P(H) \\prod_{i=1}^n P(x_i | H) }\\$$\n",
    "<br>\n",
    "\n",
    "In plain English, using Bayesian probability terminology, the above equation can be written as:\n",
    "\n",
    "$${\\displaystyle {\\text{posterior}}={\\frac {{\\text{prior}}\\times {\\text{likelihood}}}{\\text{evidence}}}\\,}$$\n",
    "\n",
    "\n",
    "prior for a given class is then:\n",
    "$${\\displaystyle {\\text{prior for a given class}}={\\frac {\\text{no. of samples in that class}}{\\text{total no. of samples}}}\\,}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the context of our spam classifier I will apply the Naive Bayes formula as follows:\n",
    "\n",
    "1. Compute the total probability of spam emails based on hand-labelled data - prior\n",
    "$$ P(S) = \\frac{|spam\\, emails|} {(|spam\\, emails|\\, +\\, |ham\\, emails|)}$$\n",
    "<br>\n",
    "2. Compute the total probability of ham emails based on hand-labelled data\n",
    "$$ P(H) = \\frac{|ham\\, emails|} {(|spam\\, emails|\\, +\\, |ham\\, emails|)}$$\n",
    "<br>\n",
    "\n",
    "1. Iterate over the previous labelled spam emails and, for each word w in the entire training set, count how many\n",
    "of the spam emails contain w. Compute conditional probability:\n",
    "$$P(w | S) = \\frac{|spam\\, emails\\, containing\\, w|+1)} {|spam\\, emails|\\, +\\, 2} $$\n",
    "<br>\n",
    "2. Compute P(w | H) the same way for ham emails.\n",
    "$$P(w | H) = \\frac{|ham\\, emails\\, containing\\, w|+1)} {|ham\\, emails|\\, +\\, 2} $$\n",
    "<br><br>\n",
    "5. Given a set of new unlabelled test emails, iterate over each email and:\n",
    "- Create a set {x1, . . . , xn} of the distinct words in the email. Ignore the words that you haven’t\n",
    "seen in the labelled training data.\n",
    "- Compute<br>\n",
    "$$P(S | x1, . . . , xn) ≈ \\frac{P(S) \\prod_{i=1}^n P(x_i | S)}{P(S) \\prod_{i=1}^n P(x_i | S)\\quad + \\quad P(H) \\prod_{i=1}^n P(x_i | H) }\\$$\n",
    "<br>\n",
    "- If P(S | x1, . . . , xn) > defined_threshold, email is considered as \"spam”, otherwise it is \"ham\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, let's set a couple of constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants\n",
    "\n",
    "# Key name for new unlabelled spam emails\n",
    "SPAM_DICT_KEY = 'spam'\n",
    "\n",
    "# Key name for new unlabelled ham emails\n",
    "HAM_DICT_KEY = 'ham'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bayes_Spam_Filter():\n",
    "    '''\n",
    "    Class implementing the Naive Bayes Filter used for computing probability of an email being a SPAM\n",
    "    '''\n",
    "    \n",
    "    # Constants used by class methods\n",
    "\n",
    "    # Laplace smoothing - the \"alpha\" parameter added to numerator when multiplying\n",
    "    # probabilities to avoid zero result for unknown words\n",
    "    SMOOTH_ALPHA = 1\n",
    "\n",
    "    # Laplace smoothing - the \"K\" parameter added to denominator to avoid division by zero\n",
    "    # It is set to \"2\" because we have 2 features - SPAM and HAM\n",
    "    SMOOTH_K = 2\n",
    "    \n",
    "    # A simple list of unimportant (stop) words\n",
    "    # The user can set whether they should be removed from the algorithm or not\n",
    "    UNIMPORTANT_WORDS = ['us', 'the', 'of', 'your', 'our']\n",
    "\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        '''\n",
    "        Constructor - no parameters expected\n",
    "        Initializes variables and builds structures that are used later during the learning process\n",
    "        \n",
    "        ::Params - none\n",
    "        '''\n",
    "    \n",
    "        # Numbers of emails\n",
    "        self.__nb_train_emails_spam = 0\n",
    "        self.__nb_train_emails_ham = 0\n",
    "        self.__nb_train_emails_all = 0\n",
    "\n",
    "        # Under normal circumstances it would be better to define asll variables as private (__x)\n",
    "        # but I want to have public access to them for printing results\n",
    "        # and creating getters does not make much sense for values or dictionaries\n",
    "\n",
    "        # General probability of spam and ham based on train data evidence\n",
    "        self.pr_s_prior = 0\n",
    "        self.pr_w_prior = 0\n",
    "\n",
    "        # Dictionary with spamicity - the probability of each word occurring in spam emails based on evidence\n",
    "        # This dictionary is structured as \"{word1: spamicity, word2: spamicity, etc.}\"\n",
    "        self.word_spamicity = {}\n",
    "\n",
    "        # Dictionary with hamicity - the probability of each word occurring in ham emails based on evidence\n",
    "        # This dictionary is structured as \"{word1: hamicity, word2: hamicity, etc.}\"\n",
    "        self.word_hamicity = {}\n",
    "\n",
    "\n",
    "       \n",
    "    def learn(self, train_emails_spam: list[str], train_emails_ham: list[str]) -> None:\n",
    "        '''\n",
    "        This method implements the learning process based on the input spam and ham emails.\n",
    "        It builds all necessary variables and structures used for computing bayes probability.\n",
    "        \n",
    "        ::Params\n",
    "        train_emails_spam - list of emails labeled as SPAM\n",
    "        train_emails_ham  - list of emails labeled as HAM\n",
    "        '''\n",
    "        \n",
    "        # Number of emails in train lists of emails\n",
    "        self.__nb_train_emails_spam = len(train_emails_spam)\n",
    "        self.__nb_train_emails_ham = len(train_emails_ham)\n",
    "        self.__nb_train_emails_all = self.__nb_train_emails_spam + self.__nb_train_emails_ham\n",
    "\n",
    "        # General probability of spam and ham based on train data evidence\n",
    "        self.pr_s_prior = self.__nb_train_emails_spam / self.__nb_train_emails_all\n",
    "        self.pr_h_prior = self.__nb_train_emails_ham / self.__nb_train_emails_all\n",
    "\n",
    "        # Building the dictionary with spamicity for each word in SPAM emails\n",
    "        self.build_word_probs(train_emails_spam, self.word_spamicity)\n",
    "\n",
    "        # Building the dictionary with hamicity for each word in HAM emails\n",
    "        self.build_word_probs(train_emails_ham, self.word_hamicity)\n",
    "\n",
    "\n",
    "        \n",
    "    def build_word_counts(self, email_list: list[str]) -> dict:\n",
    "        '''\n",
    "        Returns a dictionary of \"word:frequency\" items based on the given list of emails.\n",
    "        Each unique word from the email_list is a dictionary \"key\" and the value for that key\n",
    "        represents the number of occurrences of this word in the whole email_list.\n",
    "        All words are changed to lowercase.\n",
    "\n",
    "        ::Params\n",
    "        email_list: A list of emails represented as list[str] to build the dictionary on \n",
    "        ( for example: ['Your activity report','Your benefits activity'] )\n",
    "        It is assumed that words in strings are split by space\n",
    "    \n",
    "        ::Returns\n",
    "        A dictionary of \"word:frequency\" items based on the given list of emails.\n",
    "        Output for the example above: {\"activity\": 2, \"benefits\": 1, \"your: 2\", \"report\": 1}\n",
    "        '''\n",
    "        \n",
    "        # Init the dictionary\n",
    "        word_counts = {}\n",
    "        \n",
    "        # Let's loop through the list and create items\n",
    "        for word_list in [sentence.split() for sentence in email_list]:\n",
    "            for word in word_list:\n",
    "                word = word.lower()\n",
    "                if (word in word_counts):\n",
    "                    word_counts[word] += 1\n",
    "                else:\n",
    "                    word_counts[word] = 1\n",
    "                    \n",
    "        return word_counts\n",
    "    \n",
    "\n",
    "\n",
    "    def build_word_probs(self, email_list: list[str], word_probs: dict) -> None:\n",
    "        '''\n",
    "        Fills up the word_probs dictionary with items in the form of \"word:probability\" based on the given list of emails.\n",
    "        Each unique word from the email_list is a dictionary \"key\" and the value for that key\n",
    "        represents the probability of this word occurring in the email_list.\n",
    "        The probability of each word is based on the frequency of the word occurring in the list of emails.\n",
    "        Case insensitive\n",
    "\n",
    "        ::Params\n",
    "        email_list: A list of emails to learn from represented as list[str]\n",
    "        ( for example: ['Your activity report','Your benefits activity'] )\n",
    "        It is assumed that words in strings are split by space\n",
    "    \n",
    "        ::Returns\n",
    "        A dictionary of \"word:probability\" items based on the given list of emails.\n",
    "        An example: {\"activity\": 0.25, \"benefits\": 0.32}\n",
    "        '''\n",
    "        \n",
    "        # At first let's build the word counts from the given list\n",
    "        word_counts = self.build_word_counts(email_list)\n",
    "        \n",
    "        # Clearing the probabilities if there is anything in it\n",
    "        word_probs.clear()\n",
    "\n",
    "        # And this is a question here - should the denominator be \"the number of all emails\" or \"the number of all words\"?\n",
    "        # Students use a different approach here\n",
    "        denom_total = len(email_list)\n",
    "        # denom_total = sum(word_counts.values())\n",
    "        for word in word_counts:\n",
    "            # word_probs[word] = (word_counts.get(word, 0) + SMOOTH_ALPHA) / (denom_total + SMOOTH_DENOMINATOR)\n",
    "            word_probs[word] = (word_counts.get(word, 0) ) / (denom_total )\n",
    "\n",
    "\n",
    "\n",
    "    def calc_bayes_spam_prob(self, email: str, remove_stop_words: bool) -> float:\n",
    "        '''\n",
    "        Calculates the probability of the input email being a spam. Returns a float value in the range (0, 1)\n",
    "        The function is based on the Naive Bayes Theorem.\n",
    "        IMPORTANT: The algorithm has to be trained first by running the \"learn\" method.\n",
    "        Only then this function can be called.\n",
    "        \n",
    "        ::Params\n",
    "        email: the email for which to check the spam probability\n",
    "        remove_stop_words: whether the algorithm should exclude the stop words from the computations\n",
    "\n",
    "        ::Returns\n",
    "        Returns a float value representing the spam probability of the given email in the range (0, 1),\n",
    "        based on the train set of SPAM and HAM emails.\n",
    "        '''\n",
    "    \n",
    "        # Create a list of all words from the given email (in lowercase)\n",
    "        email_words = email.lower().split()\n",
    "\n",
    "        # To get a better accurracy, let's remove unseen words in train sets of SPAM or HAM emails\n",
    "        email_words = [word for word in email_words \n",
    "                            if word in self.word_spamicity.keys() or word in self.word_hamicity.keys()]        \n",
    "        \n",
    "        # Removing unimportant (stop) words - if asked\n",
    "        if (remove_stop_words):\n",
    "            email_words = [word for word in email_words if word not in self.UNIMPORTANT_WORDS]\n",
    "            \n",
    "        # This is the initial spam and ham probability based on prior evidence\n",
    "        p_s = self.pr_s_prior\n",
    "        p_h = self.pr_h_prior\n",
    "    \n",
    "        # Calculating the probability for each word and adding into a list - they get multiplied at the end \n",
    "        for word in email_words:\n",
    "            \n",
    "            # Computing p_w_s - word in SPAM\n",
    "            if word in self.word_spamicity:\n",
    "                p_w_s = self.word_spamicity[word]\n",
    "            else:\n",
    "                # Smoothing for word not seen in spam training data\n",
    "                p_w_s = self.SMOOTH_ALPHA / (self.__nb_train_emails_spam + self.SMOOTH_K)\n",
    "            \n",
    "            # And the SPAM probability of this word gets multiplied with the probabilities of previous words\n",
    "            p_s *= p_w_s\n",
    "          \n",
    "\n",
    "            # Computing p_w_h - word in HAM\n",
    "            if word in self.word_hamicity:\n",
    "                p_w_h = self.word_hamicity[word]\n",
    "            else:\n",
    "                # Smoothing for word not seen in ham training data\n",
    "                p_w_h = self.SMOOTH_ALPHA / (self.__nb_train_emails_ham + self.SMOOTH_K)\n",
    "\n",
    "            # And the HAM probability of this word gets multiplied with the probabilities of previous words            \n",
    "            p_h *= p_w_h\n",
    "\n",
    "        # Calculating the final probability\n",
    "        p_s_final = (p_s / (p_s + p_h) )\n",
    "\n",
    "        return(p_s_final)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid black\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the classifier on the new set of emails and printing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "          Test results with stop words removed\n",
      "------------------------------------------------------------------------------\n",
      "Email                               Expected   Spam prediction Result         \n",
      "----------------------------------- ---------- --------------- ---------------\n",
      "renew your password                 spam       76.92 %         SPAM (p > 50%)!\n",
      "renew your vows                     spam       40.00 %         HAM (p <= 50%)\n",
      "benefits of our account             ham        45.45 %         HAM (p <= 50%)\n",
      "the importance of physical activity ham        7.69 %          HAM (p <= 50%)\n",
      "\n",
      "\n",
      "\n",
      "          Test results with stop words included\n",
      "------------------------------------------------------------------------------\n",
      "Email                               Expected   Spam prediction Result         \n",
      "----------------------------------- ---------- --------------- ---------------\n",
      "renew your password                 spam       88.24 %         SPAM (p > 50%)!\n",
      "renew your vows                     spam       60.00 %         SPAM (p > 50%)!\n",
      "benefits of our account             ham        51.02 %         SPAM (p > 50%)!\n",
      "the importance of physical activity ham        4.00 %          HAM (p <= 50%)\n",
      "\n",
      "\n",
      "\n",
      "Prior probabilities\n",
      "--------------------\n",
      "Spam: 0.5714\n",
      "Ham : 0.4286\n",
      "\n",
      "\n",
      "\n",
      "Conditional probabilities (spamicity and hamicity):\n",
      "---------------------------------------------------\n",
      "Word            Spamicity       Hamicity       \n",
      "--------------- --------------- ---------------\n",
      "website         0.2500                         \n",
      "vows                            0.3333         \n",
      "send            0.7500                         \n",
      "activity                        0.6667         \n",
      "us              0.5000                         \n",
      "account         0.2500                         \n",
      "the                             0.3333         \n",
      "review          0.2500                         \n",
      "password        0.5000                         \n",
      "your            0.7500          0.3333         \n",
      "our             0.2500                         \n",
      "report                          0.3333         \n",
      "physical                        0.3333         \n",
      "importance                      0.3333         \n",
      "benefits                        0.3333         \n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of the Naive Bayes filter\n",
    "bayes_spam_filter = Bayes_Spam_Filter()\n",
    "\n",
    "# Running the learning process on the trained emails\n",
    "bayes_spam_filter.learn(previous_spam, previous_ham)\n",
    "\n",
    "# Just for printing purposes\n",
    "max_email_len = max([len(message) for emails in new_emails.values() for message in emails])\n",
    "\n",
    "# We are running two tests - one for removed stop-words and one with them included\n",
    "for remove_stop_words in (True, False):\n",
    "\n",
    "    if (remove_stop_words):\n",
    "        print('\\n          Test results with stop words removed')\n",
    "    else:\n",
    "        print('\\n\\n\\n          Test results with stop words included')  \n",
    "\n",
    "    print((max_email_len+43)*'-')\n",
    "    print(f'{\"Email\":{max_email_len}} {\"Expected\":10} {\"Spam prediction\":15} {\"Result\":15}')\n",
    "    print((max_email_len * '-') + ' ' + '-' * 10 + ' ' + '-' * 15 + ' ' + '-' * 15)\n",
    "    # Checking allegedly SPAM emails and their probability of being a spam\n",
    "    for new_email in new_emails[SPAM_DICT_KEY]:\n",
    "        p_s = bayes_spam_filter.calc_bayes_spam_prob(new_email, remove_stop_words)\n",
    "        p_s_perc = f\"{(p_s * 100.0):.2f} %\"\n",
    "        print(f'{new_email:{max_email_len}} {\"spam\":10} {p_s_perc:15} {\"SPAM (p > 50%)!\" if p_s > 0.5 else \"HAM (p <= 50%)\"}')\n",
    "\n",
    "    # Checking allegedly HAM emails and their probability of being a spam\n",
    "    for new_email in new_emails[HAM_DICT_KEY]:\n",
    "        p_s = bayes_spam_filter.calc_bayes_spam_prob(new_email, remove_stop_words)\n",
    "        p_s_perc = f\"{(p_s * 100.0):.2f} %\"\n",
    "        print(f'{new_email:{max_email_len}} {\"ham\":10} {p_s_perc:15} {\"SPAM (p > 50%)!\" if p_s > 0.5 else \"HAM (p <= 50%)\"}')\n",
    "\n",
    "        \n",
    "# Printing prior probabilities\n",
    "print(\"\\n\\n\")\n",
    "print(\"Prior probabilities\")\n",
    "print(20 * '-')\n",
    "print(f'Spam: {bayes_spam_filter.pr_s_prior:.4f}')\n",
    "print(f'Ham : {bayes_spam_filter.pr_h_prior:.4f}')\n",
    "        \n",
    "        \n",
    "# Printing conditional word probabilities\n",
    "print(\"\\n\\n\")\n",
    "print(\"Conditional probabilities (spamicity and hamicity):\")\n",
    "print(51 * '-')\n",
    "print(f'{\"Word\":15} {\"Spamicity\":15} {\"Hamicity\":15}')\n",
    "print((15 * '-') + ' ' + '-' * 15 + ' ' + '-' * 15)\n",
    "\n",
    "\n",
    "# Combining all words from spamicity and hamicity\n",
    "words_spam = set(bayes_spam_filter.word_spamicity.keys())\n",
    "words_ham  = set(bayes_spam_filter.word_hamicity.keys())\n",
    "words_all = words_spam.union(words_ham)\n",
    "\n",
    "for word in words_all:\n",
    "    spamicity = bayes_spam_filter.word_spamicity.get(word, \"\")\n",
    "    spamicity_f = (f'{spamicity:.4f}' if spamicity else \"\")\n",
    "    hamicity = bayes_spam_filter.word_hamicity.get(word, \"\")\n",
    "    hamicity_f = (f'{hamicity:.4f}' if hamicity else \"\")\n",
    "    print(f'{word:15} {spamicity_f:15} {hamicity_f:15}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\\\"border:2px solid gray\\\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write you reflection in below cell.\n",
    "\n",
    "TO DO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
